{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchkit.pytorch_utils as ptu\n",
    "import torchsde\n",
    "from torch.nn import functional as F\n",
    "import random as rnd\n",
    "import copy as cp\n",
    "# import environments\n",
    "import envs.pomdp\n",
    "import pdb\n",
    "# import recurrent model-free RL (separate architecture)\n",
    "from policies.models.policy_rnn import ModelFreeOffPolicy_Separate_RNN as Policy_RNN\n",
    "from policies.models.policy_rnn_shared import ModelFreeOffPolicy_Shared_RNN as Policy_Shared_RNN\n",
    "from policies.models.policy_mlp import ModelFreeOffPolicy_MLP as Policy_MLP\n",
    "from tqdm import tqdm\n",
    "# import the replay buffer\n",
    "from buffers.seq_replay_buffer_vanilla import SeqReplayBuffer\n",
    "from buffers.simple_replay_buffer import SimpleReplayBuffer \n",
    "from utils import helpers as utl\n",
    "from typing import Sequence\n",
    "from read_ini import read_ini\n",
    "conf =read_ini(\"C:/Users/alexander.vasilyev/pomdp-baselines-main/configfile.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a POMDP environment: Pendulum-V (only observe the velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFreeOffPolicy_Separate_RNN(\n",
      "  (critic): Critic_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear2): Linear(in_features=32, out_features=384, bias=True)\n",
      "      )\n",
      "      (realini): MLP(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (readout): Linear(in_features=32, out_features=32, bias=False)\n",
      "    )\n",
      "    (current_shortcut_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=5, out_features=10, bias=True)\n",
      "    )\n",
      "    (qf1): FlattenMlp(\n",
      "      (fc0): Linear(in_features=42, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (qf2): FlattenMlp(\n",
      "      (fc0): Linear(in_features=42, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (critic_target): Critic_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear2): Linear(in_features=32, out_features=384, bias=True)\n",
      "      )\n",
      "      (realini): MLP(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (readout): Linear(in_features=32, out_features=32, bias=False)\n",
      "    )\n",
      "    (current_shortcut_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=5, out_features=10, bias=True)\n",
      "    )\n",
      "    (qf1): FlattenMlp(\n",
      "      (fc0): Linear(in_features=42, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (qf2): FlattenMlp(\n",
      "      (fc0): Linear(in_features=42, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (actor): Actor_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear2): Linear(in_features=32, out_features=384, bias=True)\n",
      "      )\n",
      "      (realini): MLP(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (readout): Linear(in_features=32, out_features=32, bias=False)\n",
      "    )\n",
      "    (current_observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "    (policy): DeterministicPolicy(\n",
      "      (fc0): Linear(in_features=40, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (actor_target): Actor_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=2, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (linear2): Linear(in_features=32, out_features=384, bias=True)\n",
      "      )\n",
      "      (realini): MLP(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (readout): Linear(in_features=32, out_features=32, bias=False)\n",
      "    )\n",
      "    (current_observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "    )\n",
      "    (policy): DeterministicPolicy(\n",
      "      (fc0): Linear(in_features=40, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "total env episodes 2505 total env steps 501000\n"
     ]
    }
   ],
   "source": [
    "cuda_id = 0  # -1 if using cpu\n",
    "ptu.set_gpu_mode(torch.cuda.is_available() and cuda_id >= 0, cuda_id)\n",
    "\n",
    "env = gym.make(conf[\"env_name\"])\n",
    "max_trajectory_len = env._max_episode_steps\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]+1\n",
    "\n",
    "shared = False\n",
    "markov = False\n",
    "regularization = True\n",
    "\n",
    "if markov:\n",
    "    agent = Policy_MLP(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        algo_name=conf[\"algo_name\"],\n",
    "        dqn_layers=[128, 128],\n",
    "        policy_layers=[128, 128],\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=5e-3,\n",
    "    ).to(ptu.device)\n",
    "    encoder=\"Nan\"\n",
    "else:     \n",
    "    agent = Policy_RNN(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        encoder=conf[\"encoder\"],\n",
    "        algo_name=conf[\"algo_name\"],\n",
    "        action_embedding_size=int(conf[\"action_embedding_size\"]),\n",
    "        observ_embedding_size=int(conf[\"observ_embedding_size\"]),\n",
    "        reward_embedding_size=int(conf[\"reward_embedding_size\"]),\n",
    "        rnn_hidden_size=int(conf[\"hidden_size\"]),\n",
    "        dqn_layers=[128, 128],\n",
    "        policy_layers=[128, 128],\n",
    "        lr=float(conf[\"lr\"]),\n",
    "        gamma=0.9,\n",
    "        tau=0.005,\n",
    "        radii=60,\n",
    "        activation = conf[\"activation\"],\n",
    "        ini_regularization = regularization,\n",
    "    ).to(ptu.device)\n",
    "    \n",
    "print(agent)\n",
    "lr=float(conf[\"lr\"])\n",
    "encoder=conf[\"encoder\"]\n",
    "num_updates_per_iter = int(conf[\"num_updates_per_iter\"])  # training frequency\n",
    "sampled_seq_len = int(conf[\"sampled_seq_len\"])  # context length\n",
    "buffer_size = int(float(conf[\"buffer_size\"]))\n",
    "batch_size = int(conf[\"batch_size\"])\n",
    "dropout_rate=float(conf[\"dropout_rate\"])\n",
    "num_iters = int(conf[\"num_iters\"])\n",
    "num_init_rollouts_pool = int(conf[\"num_init_rollouts_pool\"])\n",
    "num_rollouts_per_iter = int(conf[\"num_rollouts_per_iter\"])\n",
    "total_rollouts = num_init_rollouts_pool + num_iters * num_rollouts_per_iter\n",
    "n_env_steps_total = max_trajectory_len * total_rollouts\n",
    "_n_env_steps_total = 0\n",
    "print(\"total env episodes\", total_rollouts, \"total env steps\", n_env_steps_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurent model-free RL agent: separate architecture, `lstm` encoder, `oar` policy input space, `td3` RL algorithm (context length set later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define other training parameters such as context length and training frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define key functions: collect rollouts and policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ncde_row(obs, next_obs, prev_action, action, prev_reward, reward, steps,non_drops,init, drop=True):\n",
    "    \n",
    "    if init:\n",
    "        obs_row= obs\n",
    "        rew_row = prev_reward\n",
    "        act_row = prev_action\n",
    "    else:\n",
    "        obs_row=torch.cat((obs, next_obs),0)\n",
    "        rew_row=torch.cat((prev_reward, reward),0)\n",
    "        act_row=torch.cat((prev_action, action),0)\n",
    "    #pdb.set_trace()\n",
    "    if shared: \n",
    "        obs_row=agent.observ_embedder(obs_row)\n",
    "        rew_row=agent.reward_embedder(rew_row)\n",
    "        act_row=agent.action_embedder(act_row)\n",
    "    else: \n",
    "        obs_row=agent.actor.observ_embedder(obs_row)\n",
    "        rew_row=agent.actor.reward_embedder(rew_row)\n",
    "        act_row=agent.actor.action_embedder(act_row)\n",
    "        #pdb.set_trace()\n",
    "    if init:\n",
    "        time_tensor=torch.tensor([[steps]]).to(ptu.device)\n",
    "        drop_tensor=torch.tensor([[non_drops]]).to(ptu.device)\n",
    "    else:\n",
    "        time_tensor=torch.tensor([[steps],[steps+1]]).to(ptu.device)\n",
    "        if drop:\n",
    "            drop_tensor=torch.tensor([[non_drops],[non_drops+1]]).to(ptu.device)\n",
    "        else:\n",
    "            drop_tensor=torch.tensor([[non_drops],[non_drops]]).to(ptu.device)\n",
    "            \n",
    "          \n",
    "    ncde_row=torch.cat((time_tensor,drop_tensor,act_row,obs_row),1)\n",
    "    ncde_row=ncde_row[None,:]\n",
    "   \n",
    "    return ncde_row\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    num_rollouts, dropout_rate=0.0, random_actions=False, deterministic=True, train_mode=True\n",
    "):\n",
    "    \"\"\"collect num_rollouts of trajectories in task and save into policy buffer\n",
    "    :param\n",
    "        random_actions: whether to use policy to sample actions, or randomly sample action space\n",
    "        deterministic: deterministic action selection?\n",
    "        train_mode: whether to train (stored to buffer) or test\n",
    "    \"\"\"\n",
    "    if not train_mode:\n",
    "        assert random_actions == False and deterministic == True\n",
    "\n",
    "    total_steps = 0\n",
    "    total_rewards = 0.0\n",
    "    trewards =[]\n",
    "    for idx in range(num_rollouts):\n",
    "        steps = 0\n",
    "        rewards = 0.0\n",
    "        energy = 0.0\n",
    "        non_drops=0\n",
    "        obs = ptu.from_numpy(env.reset())\n",
    "        #pdb.set_trace()\n",
    "        obs = obs.reshape(1, obs.shape[-1])\n",
    "        done_rollout = False\n",
    "        init=True\n",
    "        drop=True\n",
    "        # get hidden state at timestep=0, None for mlp\n",
    "        \n",
    "        if not markov:\n",
    "            action, reward, internal_state = agent.get_initial_info()\n",
    "            obs=torch.cat((obs,reward),dim=1)\n",
    "            if encoder == \"ncde\":\n",
    "                internal_state= None\n",
    "                #obs=torch.cat((obs,reward))\n",
    "                ncde_row= create_ncde_row(obs, obs, action, action, reward, reward, steps,non_drops,init, drop)\n",
    "                prev_action= action.clone()\n",
    "                prev_reward= reward.clone()\n",
    "                next_obs= obs.clone()\n",
    "        \n",
    "        \n",
    "        if train_mode:\n",
    "            # temporary storage\n",
    "            obs_list, act_list, rew_list, next_obs_list, term_list, drop_list = (\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "                           \n",
    "\n",
    "        while not done_rollout:\n",
    "            if markov: \n",
    "                action = agent.act(obs=obs, \n",
    "                                   deterministic=deterministic)[0]\n",
    "            else:\n",
    "                if encoder == \"ncde\":\n",
    "                    (action,_,_,_), internal_state= agent.ncde_act(ncde_row=ncde_row,\n",
    "                                                                   prev_internal_state=internal_state,\n",
    "                                                                   obs=obs,\n",
    "                                                                   deterministic=deterministic,)\n",
    "                else:\n",
    "                    (action, _, _, _), internal_state = agent.act(\n",
    "                        prev_internal_state=internal_state,\n",
    "                        prev_action=action,\n",
    "                        reward=reward,\n",
    "                        obs=obs,\n",
    "                        drop=drop,\n",
    "                        deterministic=deterministic,\n",
    "                    )\n",
    "            # observe reward and next obs (B=1, dim)\n",
    "            #pdb.set_trace()\n",
    "        \n",
    "            #print(torch.norm(internal_state))\n",
    "            next_obs, reward, done, info = utl.env_step(env, action.squeeze(dim=0))\n",
    "            next_obs=torch.cat((next_obs,reward),dim=1)\n",
    "            done_rollout = False if ptu.get_numpy(done[0][0]) == 0.0 else True\n",
    "            init=False\n",
    "            #print(reward)\n",
    "            #switch on/off dropouts\n",
    "            if dropout_rate>0:\n",
    "                drop_trigger=rnd.uniform(0,1)\n",
    "                if drop_trigger<dropout_rate:\n",
    "\n",
    "                    next_obs= obs.clone()\n",
    "                    drop=False\n",
    "                else:\n",
    "                    drop=True\n",
    "                    \n",
    "                    \n",
    "            if not markov:\n",
    "                if encoder == \"ncde\":  \n",
    "                    ncde_row= create_ncde_row(obs, next_obs, prev_action, action, prev_reward, reward, steps,non_drops,init, drop)\n",
    "            \n",
    "            #switch on/off dropouts\n",
    "\n",
    "            # update statistics\n",
    "           \n",
    "            rewards += reward.item()\n",
    "            energy += action*action\n",
    "           \n",
    "            # early stopping env: such as rmdp, pomdp, generalize tasks. term ignores timeout\n",
    "            term = (\n",
    "                False\n",
    "                if \"TimeLimit.truncated\" in info or steps >= max_trajectory_len\n",
    "                else done_rollout\n",
    "            )\n",
    "\n",
    "            if train_mode:\n",
    "                # append tensors to temporary storage\n",
    "                obs_list.append(obs)  # (1, dim)\n",
    "                act_list.append(action)  # (1, dim)\n",
    "                rew_list.append(reward)  # (1, dim)\n",
    "                term_list.append(term)  # bool\n",
    "                next_obs_list.append(next_obs)  # (1, dim)\n",
    "            steps += 1\n",
    "            # set: obs <- next_obs\n",
    "            obs = next_obs.clone()\n",
    "            prev_reward= reward.clone()\n",
    "            prev_action= action.clone()\n",
    "            if drop:\n",
    "                non_drops +=1\n",
    "        if train_mode:\n",
    "            # add collected sequence to buffer\n",
    "            policy_storage.add_episode(\n",
    "                observations=ptu.get_numpy(torch.cat(obs_list, dim=0)),  # (L, dim)\n",
    "                actions=ptu.get_numpy(torch.cat(act_list, dim=0)),  # (L, dim)\n",
    "                rewards=ptu.get_numpy(torch.cat(rew_list, dim=0)),  # (L, dim)\n",
    "                terminals=np.array(term_list).reshape(-1, 1),  # (L, 1)\n",
    "                next_observations=ptu.get_numpy(\n",
    "                    torch.cat(next_obs_list, dim=0)\n",
    "                ),  # (L, dim)\n",
    "            )\n",
    "        print(\n",
    "            \"Mode:\",\n",
    "            \"Train\" if train_mode else \"Test\",\n",
    "            \"env_steps\",\n",
    "            steps,\n",
    "            \"total rewards\",\n",
    "            rewards,\n",
    "            \"total energy\",\n",
    "            energy,\n",
    "        )\n",
    "        total_steps += steps\n",
    "        total_rewards += rewards\n",
    "        trewards.append(rewards)\n",
    "    if train_mode:\n",
    "        return total_steps\n",
    "    else:\n",
    "        return total_rewards / num_rollouts, np.std(trewards)\n",
    "\n",
    "\n",
    "def update(num_updates, factor):\n",
    "    rl_losses_agg = {}\n",
    "    # print(num_updates)\n",
    "    for update in tqdm(range(num_updates), leave=True):\n",
    "        # sample random RL batch: in transitions\n",
    "        batch = ptu.np_to_pytorch_batch(policy_storage.random_episodes(batch_size))\n",
    "        # RL update\n",
    "        \n",
    "        rl_losses = agent.update(batch, factor)\n",
    "\n",
    "        for k, v in rl_losses.items():\n",
    "            if update == 0:  # first iterate - create list\n",
    "                rl_losses_agg[k] = [v]\n",
    "            else:  # append values\n",
    "                rl_losses_agg[k].append(v)\n",
    "    # statistics\n",
    "    for k in rl_losses_agg:\n",
    "        rl_losses_agg[k] = np.mean(rl_losses_agg[k])\n",
    "    return rl_losses_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer RAM usage: 0.04 GB\n",
      "Mode: Train env_steps 200 total rewards -1386.3829669952393 total energy tensor([[0.0005]])\n",
      "Mode: Train env_steps 200 total rewards -1828.3410663604736 total energy tensor([[0.0005]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11424\\2431316952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     env_steps = collect_rollouts(\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mnum_rollouts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_init_rollouts_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_actions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11424\\2471452667.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(num_rollouts, dropout_rate, random_actions, deterministic, train_mode)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ncde\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                     (action,_,_,_), internal_state= agent.ncde_act(ncde_row=ncde_row,\n\u001b[0m\u001b[0;32m     99\u001b[0m                                                                    \u001b[0mprev_internal_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minternal_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                                                                    \u001b[0mobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\pomdp-baselines-main\\policies\\models\\policy_rnn.py\u001b[0m in \u001b[0;36mncde_act\u001b[1;34m(self, ncde_row, prev_internal_state, obs, deterministic, return_log_prob)\u001b[0m\n\u001b[0;32m    160\u001b[0m     ):\n\u001b[0;32m    161\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0mcurrent_action_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_internal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mncde_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncde_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_internal_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_log_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\pomdp-baselines-main\\policies\\models\\recurrent_actor.py\u001b[0m in \u001b[0;36mncde_act\u001b[1;34m(self, ncde_row, prev_internal_state, obs, deterministic, return_log_prob)\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mhidden_state\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_internal_state\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mcurrent_internal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncde_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_internal_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[0mhidden_state\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_internal_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mhidden_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\pomdp-baselines-main\\torchkit\\networks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, coeffs, init_hid)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mz0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_hid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         z_T = torchcde.cdeint(X=X,\n\u001b[0m\u001b[0;32m    245\u001b[0m                               \u001b[0mz0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                               \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchcde\\solver.py\u001b[0m in \u001b[0;36mcdeint\u001b[1;34m(X, func, z0, t, adjoint, backend, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"adjoint_rtol\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rtol\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[0mis_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_prod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_compatability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0madjoint\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'adjoint_params'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchcde\\solver.py\u001b[0m in \u001b[0;36m_check_compatability\u001b[1;34m(X, func, z0, t)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'derivative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X must have a 'derivative' method.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mcontrol_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prod'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mis_prod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropouts=0.0\n",
    "learning_curves=[]\n",
    "while dropouts<1:\n",
    "    _n_env_steps_total =0 \n",
    "    \n",
    "    policy_storage = SeqReplayBuffer(\n",
    "        max_replay_buffer_size=buffer_size,\n",
    "        observation_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        sampled_seq_len=sampled_seq_len,\n",
    "        sample_weight_baseline=0.0,\n",
    "    )\n",
    "\n",
    "    env_steps = collect_rollouts(\n",
    "        num_rollouts=num_init_rollouts_pool, dropout_rate=dropouts, random_actions=False, train_mode=True\n",
    "    )\n",
    "    _n_env_steps_total += env_steps\n",
    "\n",
    "    # evaluation parameters\n",
    "    last_eval_num_iters = 10\n",
    "    log_interval = 5\n",
    "    eval_num_rollouts = 10\n",
    "    learning_curve = {\n",
    "        \"x\": [],\n",
    "        \"y\": [],\n",
    "        \"z\": [],\n",
    "    }\n",
    "    epoch=0\n",
    "    lambda_pat = 0.65\n",
    "    \n",
    "    while _n_env_steps_total < n_env_steps_total:\n",
    "\n",
    "        env_steps = collect_rollouts(num_rollouts=num_rollouts_per_iter, dropout_rate=dropouts, train_mode=True)\n",
    "        _n_env_steps_total += env_steps\n",
    "\n",
    "        #train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "        factor= lambda_pat **(epoch )\n",
    "        #train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "        train_stats = update(25, lr)\n",
    "\n",
    "        epoch += 1\n",
    "        current_num_iters = _n_env_steps_total // (\n",
    "            num_rollouts_per_iter * max_trajectory_len\n",
    "        )\n",
    "        if (\n",
    "            current_num_iters != last_eval_num_iters\n",
    "            and current_num_iters % log_interval == 0\n",
    "        ):\n",
    "            last_eval_num_iters = current_num_iters\n",
    "            average_returns, std_returns = collect_rollouts(\n",
    "                num_rollouts=eval_num_rollouts, dropout_rate=dropouts,\n",
    "                train_mode=False,\n",
    "                random_actions=False,\n",
    "                deterministic=True,\n",
    "            )\n",
    "            learning_curve[\"x\"].append(_n_env_steps_total)\n",
    "            learning_curve[\"y\"].append(average_returns)\n",
    "            learning_curve[\"z\"].append(std_returns)\n",
    "            print(_n_env_steps_total, average_returns)\n",
    "    learning_curves.append(learning_curve)\n",
    "    dropouts+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the agent: only costs < 20 min"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(learning_curve)\n",
    "plt.plot(learning_curves[0][\"x\"], learning_curves[0][\"y\"])\n",
    "plt.fill_between(np.array(learning_curves[0][\"x\"]), np.array(learning_curves[0][\"y\"])-np.array(learning_curves[0][\"z\"]), np.array(learning_curves[0][\"y\"])+np.array(learning_curves[0][\"z\"]))\n",
    "plt.xlabel(\"env steps\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaning_ncde_05= learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=np.array(learning_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timess=torch.linspace(0, 65-1, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('config.txt', 'w')\n",
    "file1.write(str(conf))\n",
    "\n",
    "file1.close()\n",
    "file2 = open('results.txt', 'w')\n",
    "file2.write(str(learning_curves))\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(learning_curve)\n",
    "plt.plot(learning_curves[0][\"x\"], learning_curves[0][\"y\"], label = \"lstm_0drop\")\n",
    "plt.legend(loc = \"lower right\", fontsize = \"large\")\n",
    "plt.plot(learning_curves[1][\"x\"], learning_curves[1][\"y\"], label = \"lstm_02drop\")\n",
    "plt.legend(loc = \"lower right\", fontsize = \"large\")\n",
    "plt.fill_between(np.array(learning_curves[0][\"x\"]), np.array(learning_curves[0][\"y\"])-np.array(learning_curves[0][\"z\"]), np.array(learning_curves[0][\"y\"])+np.array(learning_curves[0][\"z\"]))\n",
    "plt.fill_between(np.array(learning_curves[1][\"x\"]), np.array(learning_curves[1][\"y\"])-np.array(learning_curves[1][\"z\"]), np.array(learning_curves[1][\"y\"])+np.array(learning_curves[1][\"z\"]))\n",
    "plt.xlabel(\"env steps\", fontsize = \"x-large\")\n",
    "plt.ylabel(\"return\", fontsize = \"x-large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(learning_curve)\n",
    "plt.plot(learning_curves[2][\"x\"], learning_curves[2][\"y\"], label = \"lstm_04drop\")\n",
    "plt.legend(loc = \"lower right\", fontsize = \"large\")\n",
    "plt.plot(learning_curves[3][\"x\"], learning_curves[3][\"y\"], label = \"lstm_06drop\")\n",
    "plt.legend(loc = \"lower right\", fontsize = \"large\")\n",
    "plt.fill_between(np.array(learning_curves[2][\"x\"]), np.array(learning_curves[2][\"y\"])-np.array(learning_curves[2][\"z\"]), np.array(learning_curves[2][\"y\"])+np.array(learning_curves[2][\"z\"]))\n",
    "plt.fill_between(np.array(learning_curves[3][\"x\"]), np.array(learning_curves[3][\"y\"])-np.array(learning_curves[3][\"z\"]), np.array(learning_curves[3][\"y\"])+np.array(learning_curves[3][\"z\"]))\n",
    "plt.xlabel(\"env steps\", fontsize = \"x-large\")\n",
    "plt.ylabel(\"return\", fontsize = \"x-large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(learning_curve)\n",
    "plt.plot(learning_curves[4][\"x\"], learning_curves[4][\"y\"], label = \"lstm_04drop\")\n",
    "plt.legend(loc = \"lower right\", fontsize = \"large\")\n",
    "plt.plot(learning_curves[5][\"x\"], learning_curves[5][\"y\"], label = \"lstm_06drop\")\n",
    "plt.legend(loc = \"lower right\", fontsize = \"large\")\n",
    "plt.fill_between(np.array(learning_curves[4][\"x\"]), np.array(learning_curves[4][\"y\"])-np.array(learning_curves[4][\"z\"]), np.array(learning_curves[4][\"y\"])+np.array(learning_curves[4][\"z\"]))\n",
    "plt.fill_between(np.array(learning_curves[5][\"x\"]), np.array(learning_curves[5][\"y\"])-np.array(learning_curves[5][\"z\"]), np.array(learning_curves[5][\"y\"])+np.array(learning_curves[5][\"z\"]))\n",
    "plt.xlabel(\"env steps\", fontsize = \"x-large\")\n",
    "plt.ylabel(\"return\", fontsize = \"x-large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5f49fb63f78fde0f27b95a7c8e14eeaa9af6d816174ff450f7bbbcd21c7c97c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
