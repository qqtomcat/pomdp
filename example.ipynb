{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexander.vasilyev\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\alexander.vasilyev\\pomdp-baselines-main\\utils\\logger.py:9: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import OrderedDict, Set\n",
      "C:\\Users\\alexander.vasilyev\\Anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchkit.pytorch_utils as ptu\n",
    "import torchsde\n",
    "from torch.nn import functional as F\n",
    "import random as rnd\n",
    "import copy as cp\n",
    "# import environments\n",
    "import envs.pomdp\n",
    "import pdb\n",
    "# import recurrent model-free RL (separate architecture)\n",
    "from policies.models.policy_rnn import ModelFreeOffPolicy_Separate_RNN as Policy_RNN\n",
    "from policies.models.policy_rnn_shared import ModelFreeOffPolicy_Shared_RNN as Policy_Shared_RNN\n",
    "from policies.models.policy_mlp import ModelFreeOffPolicy_MLP as Policy_MLP\n",
    "from tqdm import tqdm\n",
    "# import the replay buffer\n",
    "from buffers.seq_replay_buffer_vanilla import SeqReplayBuffer\n",
    "from buffers.simple_replay_buffer import SimpleReplayBuffer \n",
    "from utils import helpers as utl\n",
    "from typing import Sequence\n",
    "from read_ini import read_ini\n",
    "conf =read_ini(\"C:/Users/alexander.vasilyev/pomdp-baselines-main/configfile.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a POMDP environment: Pendulum-V (only observe the velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFreeOffPolicy_Separate_RNN(\n",
      "  (critic): Critic_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_shortcut_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=2, out_features=48, bias=True)\n",
      "    )\n",
      "    (qf1): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (qf2): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (critic_target): Critic_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_shortcut_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=2, out_features=48, bias=True)\n",
      "    )\n",
      "    (qf1): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (qf2): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (actor): Actor_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (policy): DeterministicPolicy(\n",
      "      (fc0): Linear(in_features=104, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (actor_target): Actor_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (policy): DeterministicPolicy(\n",
      "      (fc0): Linear(in_features=104, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "total env episodes 2505 total env steps 501000\n"
     ]
    }
   ],
   "source": [
    "cuda_id = 0  # -1 if using cpu\n",
    "ptu.set_gpu_mode(torch.cuda.is_available() and cuda_id >= 0, cuda_id)\n",
    "\n",
    "env = gym.make(conf[\"env_name\"])\n",
    "max_trajectory_len = env._max_episode_steps\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "shared = False\n",
    "markov = False\n",
    "\n",
    "if markov:\n",
    "    agent = Policy_MLP(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        algo_name=conf[\"algo_name\"],\n",
    "        dqn_layers=[128, 128],\n",
    "        policy_layers=[128, 128],\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=5e-3,\n",
    "    ).to(ptu.device)\n",
    "    encoder=\"Nan\"\n",
    "else:\n",
    "    if shared:\n",
    "        agent = Policy_Shared_RNN(\n",
    "            obs_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            encoder=conf[\"encoder\"],\n",
    "            algo_name=conf[\"algo_name\"],\n",
    "            action_embedding_size=int(conf[\"action_embedding_size\"]),\n",
    "            observ_embedding_size=int(conf[\"observ_embedding_size\"]),\n",
    "            reward_embedding_size=int(conf[\"reward_embedding_size\"]),\n",
    "            rnn_hidden_size=int(conf[\"hidden_size\"]),\n",
    "            dqn_layers=[128, 128],\n",
    "            policy_layers=[128, 128],\n",
    "            lr=float(conf[\"lr\"]),\n",
    "            gamma=0.9,\n",
    "            tau=0.005,\n",
    "            embed=True,\n",
    "        ).to(ptu.device)\n",
    "    else: \n",
    "        agent = Policy_RNN(\n",
    "            obs_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            encoder=conf[\"encoder\"],\n",
    "            algo_name=conf[\"algo_name\"],\n",
    "            action_embedding_size=int(conf[\"action_embedding_size\"]),\n",
    "            observ_embedding_size=int(conf[\"observ_embedding_size\"]),\n",
    "            reward_embedding_size=int(conf[\"reward_embedding_size\"]),\n",
    "            rnn_hidden_size=int(conf[\"hidden_size\"]),\n",
    "            dqn_layers=[128, 128],\n",
    "            policy_layers=[128, 128],\n",
    "            lr=float(conf[\"lr\"]),\n",
    "            gamma=0.9,\n",
    "            tau=0.005,\n",
    "            radii=40,\n",
    "            embed=True,\n",
    "            activation = conf[\"activation\"],\n",
    "        ).to(ptu.device)\n",
    "    \n",
    "print(agent)\n",
    "lr=float(conf[\"lr\"])\n",
    "encoder=conf[\"encoder\"]\n",
    "num_updates_per_iter = int(conf[\"num_updates_per_iter\"])  # training frequency\n",
    "sampled_seq_len = int(conf[\"sampled_seq_len\"])  # context length\n",
    "buffer_size = int(float(conf[\"buffer_size\"]))\n",
    "batch_size = int(conf[\"batch_size\"])\n",
    "dropout_rate=float(conf[\"dropout_rate\"])\n",
    "num_iters = int(conf[\"num_iters\"])\n",
    "num_init_rollouts_pool = int(conf[\"num_init_rollouts_pool\"])\n",
    "num_rollouts_per_iter = int(conf[\"num_rollouts_per_iter\"])\n",
    "total_rollouts = num_init_rollouts_pool + num_iters * num_rollouts_per_iter\n",
    "n_env_steps_total = max_trajectory_len * total_rollouts\n",
    "_n_env_steps_total = 0\n",
    "print(\"total env episodes\", total_rollouts, \"total env steps\", n_env_steps_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurent model-free RL agent: separate architecture, `lstm` encoder, `oar` policy input space, `td3` RL algorithm (context length set later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define other training parameters such as context length and training frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define key functions: collect rollouts and policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ncde_row(obs, next_obs, prev_action, action, prev_reward, reward, steps,init):\n",
    "    \n",
    "    if init:\n",
    "        obs_row= obs\n",
    "        rew_row = prev_reward\n",
    "        act_row = prev_action\n",
    "    else:\n",
    "        obs_row=torch.cat((obs, next_obs),0)\n",
    "        rew_row=torch.cat((prev_reward, reward),0)\n",
    "        act_row=torch.cat((prev_action, action),0)\n",
    " \n",
    "    if shared: \n",
    "        obs_row=agent.observ_embedder(obs_row)\n",
    "        rew_row=agent.reward_embedder(rew_row)\n",
    "        act_row=agent.action_embedder(act_row)\n",
    "    else: \n",
    "        obs_row=agent.actor.observ_embedder(obs_row)\n",
    "        rew_row=agent.actor.reward_embedder(rew_row)\n",
    "        act_row=agent.actor.action_embedder(act_row)\n",
    "    \n",
    "    if init:\n",
    "        time_tensor=torch.tensor([[steps]]).to(ptu.device)\n",
    "    else:\n",
    "        time_tensor=torch.tensor([[steps],[steps+1]]).to(ptu.device)\n",
    "\n",
    "    ncde_row=torch.cat((time_tensor,act_row,obs_row,rew_row),1)\n",
    "    ncde_row=ncde_row[None,:]\n",
    "    \n",
    "    return ncde_row\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    num_rollouts, random_actions=False, deterministic=True, train_mode=True\n",
    "):\n",
    "    \"\"\"collect num_rollouts of trajectories in task and save into policy buffer\n",
    "    :param\n",
    "        random_actions: whether to use policy to sample actions, or randomly sample action space\n",
    "        deterministic: deterministic action selection?\n",
    "        train_mode: whether to train (stored to buffer) or test\n",
    "    \"\"\"\n",
    "    if not train_mode:\n",
    "        assert random_actions == False and deterministic == True\n",
    "\n",
    "    total_steps = 0\n",
    "    total_rewards = 0.0\n",
    "    trewards =[]\n",
    "    for idx in range(num_rollouts):\n",
    "        steps = 0\n",
    "        rewards = 0.0\n",
    "        energy = 0.0\n",
    "        print(env.reset())\n",
    "        obs = ptu.from_numpy(env.reset())\n",
    "        obs = obs.reshape(1, obs.shape[-1])\n",
    "        done_rollout = False\n",
    "        init=True\n",
    "        # get hidden state at timestep=0, None for mlp\n",
    "        \n",
    "        if not markov:\n",
    "            action, reward, internal_state = agent.get_initial_info()\n",
    "\n",
    "            if encoder == \"ncde\":\n",
    "                internal_state= None\n",
    "                ncde_row= create_ncde_row(obs, obs, action, action, reward, reward, steps,init)\n",
    "                prev_action= action.clone()\n",
    "                prev_reward= reward.clone()\n",
    "                next_obs= obs.clone()\n",
    "        \n",
    "        \n",
    "        if train_mode:\n",
    "            # temporary storage\n",
    "            obs_list, act_list, rew_list, next_obs_list, term_list = (\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "                           \n",
    "\n",
    "        while not done_rollout:\n",
    "            if markov: \n",
    "                action = agent.act(obs=obs, deterministic=deterministic)[0]\n",
    "            else:\n",
    "                if encoder == \"ncde\":\n",
    "                    (action,_,_,_), internal_state= agent.ncde_act(ncde_row=ncde_row, prev_internal_state=internal_state, obs=obs,  deterministic=deterministic)\n",
    "                else:\n",
    "                    (action, _, _, _), internal_state = agent.act(\n",
    "                        prev_internal_state=internal_state,\n",
    "                        prev_action=action,\n",
    "                        reward=reward,\n",
    "                        obs=obs,\n",
    "                        deterministic=deterministic,\n",
    "                    )\n",
    "            # observe reward and next obs (B=1, dim)\n",
    "            #pdb.set_trace()\n",
    "        \n",
    "            #print(torch.norm(internal_state))\n",
    "            next_obs, reward, done, info = utl.env_step(env, action.squeeze(dim=0))\n",
    "            done_rollout = False if ptu.get_numpy(done[0][0]) == 0.0 else True\n",
    "            init=False\n",
    "            \n",
    "            if not markov:\n",
    "                if encoder == \"ncde\":\n",
    "   \n",
    "                    ncde_row= create_ncde_row(obs, next_obs, prev_action, action, prev_reward, reward, steps,init)\n",
    "            \n",
    "            #switch on/off dropouts\n",
    "            #drop_trigger=rnd.uniform(0,1)\n",
    "            #if drop_trigger<dropout_rate:\n",
    "            #    next_obs=cp.deepcopy(obs)\n",
    "            # update statistics\n",
    "           \n",
    "            rewards += reward.item()\n",
    "            energy += action*action\n",
    "           \n",
    "            # early stopping env: such as rmdp, pomdp, generalize tasks. term ignores timeout\n",
    "            term = (\n",
    "                False\n",
    "                if \"TimeLimit.truncated\" in info or steps >= max_trajectory_len\n",
    "                else done_rollout\n",
    "            )\n",
    "\n",
    "            if train_mode:\n",
    "                # append tensors to temporary storage\n",
    "                obs_list.append(obs)  # (1, dim)\n",
    "                act_list.append(action)  # (1, dim)\n",
    "                rew_list.append(reward)  # (1, dim)\n",
    "                term_list.append(term)  # bool\n",
    "                next_obs_list.append(next_obs)  # (1, dim)\n",
    "            steps += 1\n",
    "            # set: obs <- next_obs\n",
    "            obs = next_obs.clone()\n",
    "            prev_reward= reward.clone()\n",
    "            prev_action= action.clone()\n",
    "        if train_mode:\n",
    "            # add collected sequence to buffer\n",
    "            policy_storage.add_episode(\n",
    "                observations=ptu.get_numpy(torch.cat(obs_list, dim=0)),  # (L, dim)\n",
    "                actions=ptu.get_numpy(torch.cat(act_list, dim=0)),  # (L, dim)\n",
    "                rewards=ptu.get_numpy(torch.cat(rew_list, dim=0)),  # (L, dim)\n",
    "                terminals=np.array(term_list).reshape(-1, 1),  # (L, 1)\n",
    "                next_observations=ptu.get_numpy(\n",
    "                    torch.cat(next_obs_list, dim=0)\n",
    "                ),  # (L, dim)\n",
    "            )\n",
    "        print(\n",
    "            \"Mode:\",\n",
    "            \"Train\" if train_mode else \"Test\",\n",
    "            \"env_steps\",\n",
    "            steps,\n",
    "            \"total rewards\",\n",
    "            rewards,\n",
    "            \"total energy\",\n",
    "            energy,\n",
    "        )\n",
    "        total_steps += steps\n",
    "        total_rewards += rewards\n",
    "        trewards.append(rewards)\n",
    "    if train_mode:\n",
    "        return total_steps\n",
    "    else:\n",
    "        return total_rewards / num_rollouts, np.std(trewards)\n",
    "\n",
    "\n",
    "def update(num_updates, factor):\n",
    "    rl_losses_agg = {}\n",
    "    # print(num_updates)\n",
    "    for update in tqdm(range(num_updates), leave=True):\n",
    "        # sample random RL batch: in transitions\n",
    "        batch = ptu.np_to_pytorch_batch(policy_storage.random_episodes(batch_size))\n",
    "        # RL update\n",
    "        \n",
    "        rl_losses = agent.update(batch, factor)\n",
    "\n",
    "        for k, v in rl_losses.items():\n",
    "            if update == 0:  # first iterate - create list\n",
    "                rl_losses_agg[k] = [v]\n",
    "            else:  # append values\n",
    "                rl_losses_agg[k].append(v)\n",
    "    # statistics\n",
    "    for k in rl_losses_agg:\n",
    "        rl_losses_agg[k] = np.mean(rl_losses_agg[k])\n",
    "    return rl_losses_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the agent: only costs < 20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer RAM usage: 0.02 GB\n",
      "[-0.18286392]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexander.vasilyev\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: Train env_steps 200 total rewards -627.1723878339399 total energy tensor([[0.0009]])\n",
      "[-0.86359847]\n",
      "Mode: Train env_steps 200 total rewards -1465.099901676178 total energy tensor([[0.0006]])\n",
      "[0.74585605]\n",
      "Mode: Train env_steps 200 total rewards -968.4575101137161 total energy tensor([[0.0010]])\n",
      "[-0.8119963]\n",
      "Mode: Train env_steps 200 total rewards -1064.254163146019 total energy tensor([[0.0011]])\n",
      "[-0.65336865]\n",
      "Mode: Train env_steps 200 total rewards -1254.5659635066986 total energy tensor([[0.0009]])\n",
      "[-0.84177446]\n",
      "Mode: Train env_steps 200 total rewards -1584.4519271850586 total energy tensor([[0.0006]])\n",
      "[0.05887923]\n",
      "Mode: Train env_steps 200 total rewards -628.1147714760154 total energy tensor([[0.0009]])\n",
      "[-0.29377258]\n",
      "Mode: Train env_steps 200 total rewards -737.794536806643 total energy tensor([[0.0010]])\n",
      "[-0.8305148]\n",
      "Mode: Train env_steps 200 total rewards -1557.0814833641052 total energy tensor([[0.0006]])\n",
      "[0.61149436]\n",
      "Mode: Train env_steps 200 total rewards -1171.859337568283 total energy tensor([[0.0009]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\alexander.vasilyev\\pomdp-baselines-main\\torchkit\\pytorch_utils.py:73: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if v.dtype == np.bool:\n",
      "100%|██████████| 25/25 [01:02<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6731633]\n",
      "Mode: Train env_steps 200 total rewards -1343.8932387828827 total energy tensor([[31.7528]])\n",
      "[-0.27208433]\n",
      "Mode: Train env_steps 200 total rewards -1266.3245940208435 total energy tensor([[32.4656]])\n",
      "[0.5807534]\n",
      "Mode: Train env_steps 200 total rewards -1699.2578616142273 total energy tensor([[19.7072]])\n",
      "[-0.04527098]\n",
      "Mode: Train env_steps 200 total rewards -1307.411400437355 total energy tensor([[31.1604]])\n",
      "[-0.97157735]\n",
      "Mode: Train env_steps 200 total rewards -1351.332904547453 total energy tensor([[32.0476]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:07<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.48463875]\n",
      "Mode: Train env_steps 200 total rewards -1013.9208480650559 total energy tensor([[3.8427]])\n",
      "[0.5292008]\n",
      "Mode: Train env_steps 200 total rewards -905.1960357353091 total energy tensor([[2.5171]])\n",
      "[-0.27751896]\n",
      "Mode: Train env_steps 200 total rewards -1270.2195472717285 total energy tensor([[2.2214]])\n",
      "[0.39900678]\n",
      "Mode: Train env_steps 200 total rewards -1548.196283340454 total energy tensor([[1.6130]])\n",
      "[0.55401415]\n",
      "Mode: Train env_steps 200 total rewards -1056.3510165549815 total energy tensor([[4.3005]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:14<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8541971]\n",
      "Mode: Train env_steps 200 total rewards -1039.1114730238914 total energy tensor([[6.0354]])\n",
      "[0.41087466]\n",
      "Mode: Train env_steps 200 total rewards -1600.6446633338928 total energy tensor([[2.6838]])\n",
      "[0.76770645]\n",
      "Mode: Train env_steps 200 total rewards -959.2560813948512 total energy tensor([[5.9694]])\n",
      "[0.56193846]\n",
      "Mode: Train env_steps 200 total rewards -1045.8042680621147 total energy tensor([[6.0185]])\n",
      "[-0.4370459]\n",
      "Mode: Train env_steps 200 total rewards -1092.2935677319765 total energy tensor([[6.8191]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:10<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6180618]\n",
      "Mode: Test env_steps 200 total rewards -1662.0854816436768 total energy tensor([[14.0714]])\n",
      "[-0.8945591]\n",
      "Mode: Test env_steps 200 total rewards -1202.432815656066 total energy tensor([[18.3976]])\n",
      "[0.890028]\n",
      "Mode: Test env_steps 200 total rewards -1941.72012424469 total energy tensor([[0.5264]])\n",
      "[0.80951387]\n",
      "Mode: Test env_steps 200 total rewards -1864.3609809875488 total energy tensor([[5.9019]])\n",
      "[0.42710203]\n",
      "Mode: Test env_steps 200 total rewards -1746.466650724411 total energy tensor([[13.6720]])\n",
      "[0.74520326]\n",
      "Mode: Test env_steps 200 total rewards -1943.074589729309 total energy tensor([[0.6629]])\n",
      "[-0.25170442]\n",
      "Mode: Test env_steps 200 total rewards -1814.7647194862366 total energy tensor([[6.0244]])\n",
      "[0.27936238]\n",
      "Mode: Test env_steps 200 total rewards -1629.9115783572197 total energy tensor([[21.4080]])\n",
      "[-0.00369019]\n",
      "Mode: Test env_steps 200 total rewards -1777.9158170223236 total energy tensor([[11.5687]])\n",
      "[0.16060762]\n",
      "Mode: Test env_steps 200 total rewards -1491.5958019224927 total energy tensor([[23.8369]])\n",
      "5000 -1707.4328559773974\n",
      "[-0.02452885]\n",
      "Mode: Train env_steps 200 total rewards -1716.6345075368881 total energy tensor([[15.9769]])\n",
      "[0.07957311]\n",
      "Mode: Train env_steps 200 total rewards -1781.354884147644 total energy tensor([[7.6416]])\n",
      "[0.19677001]\n",
      "Mode: Train env_steps 200 total rewards -1744.4727334976196 total energy tensor([[13.7834]])\n",
      "[-0.22868241]\n",
      "Mode: Train env_steps 200 total rewards -1860.7293000221252 total energy tensor([[2.9837]])\n",
      "[-0.63196903]\n",
      "Mode: Train env_steps 200 total rewards -1827.9240417480469 total energy tensor([[4.2675]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:15<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9493371]\n",
      "Mode: Train env_steps 200 total rewards -1725.1589938402176 total energy tensor([[11.8734]])\n",
      "[-0.6375915]\n",
      "Mode: Train env_steps 200 total rewards -1766.9023275375366 total energy tensor([[9.1713]])\n",
      "[0.1520319]\n",
      "Mode: Train env_steps 200 total rewards -1849.0722332000732 total energy tensor([[3.8441]])\n",
      "[-0.89367926]\n",
      "Mode: Train env_steps 200 total rewards -1626.971121788025 total energy tensor([[16.1817]])\n",
      "[0.26634613]\n",
      "Mode: Train env_steps 200 total rewards -1375.0553357303143 total energy tensor([[42.9877]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:09<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77172476]\n",
      "Mode: Train env_steps 200 total rewards -1404.228477358818 total energy tensor([[56.1248]])\n",
      "[-0.14430985]\n",
      "Mode: Train env_steps 200 total rewards -1414.0377424955368 total energy tensor([[56.4431]])\n",
      "[-0.18019481]\n",
      "Mode: Train env_steps 200 total rewards -1420.5737288594246 total energy tensor([[57.7075]])\n",
      "[0.05015166]\n",
      "Mode: Train env_steps 200 total rewards -1876.0046792030334 total energy tensor([[2.0594]])\n",
      "[0.8314296]\n",
      "Mode: Train env_steps 200 total rewards -1534.730565071106 total energy tensor([[3.8204]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:12<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3954042]\n",
      "Mode: Train env_steps 200 total rewards -1383.6435743272305 total energy tensor([[94.4836]])\n",
      "[0.20333406]\n",
      "Mode: Train env_steps 200 total rewards -1355.230950385332 total energy tensor([[81.6843]])\n",
      "[0.8841586]\n",
      "Mode: Train env_steps 200 total rewards -1192.585041904822 total energy tensor([[68.4791]])\n",
      "[0.83319604]\n",
      "Mode: Train env_steps 200 total rewards -1731.3692646026611 total energy tensor([[5.9787]])\n",
      "[0.0804089]\n",
      "Mode: Train env_steps 200 total rewards -908.4952503293753 total energy tensor([[2.1642]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15499073]\n",
      "Mode: Train env_steps 200 total rewards -1607.0602875351906 total energy tensor([[163.1991]])\n",
      "[-0.19695659]\n",
      "Mode: Train env_steps 200 total rewards -1518.6114336028695 total energy tensor([[137.2252]])\n",
      "[0.4013462]\n",
      "Mode: Train env_steps 200 total rewards -1890.2161655426025 total energy tensor([[2.0544]])\n",
      "[-0.5135966]\n",
      "Mode: Train env_steps 200 total rewards -1951.1738548278809 total energy tensor([[0.4269]])\n",
      "[0.1571486]\n",
      "Mode: Train env_steps 200 total rewards -1874.922622680664 total energy tensor([[1.8949]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:32<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9871652]\n",
      "Mode: Test env_steps 200 total rewards -1388.7813239395618 total energy tensor([[108.5133]])\n",
      "[0.48804292]\n",
      "Mode: Test env_steps 200 total rewards -1611.185109257698 total energy tensor([[168.8188]])\n",
      "[0.5315943]\n",
      "Mode: Test env_steps 200 total rewards -1460.1157260090113 total energy tensor([[131.8175]])\n",
      "[-0.8156318]\n",
      "Mode: Test env_steps 200 total rewards -1585.284763097763 total energy tensor([[163.9564]])\n",
      "[0.4804839]\n",
      "Mode: Test env_steps 200 total rewards -1534.7785720825195 total energy tensor([[142.8596]])\n",
      "[0.24370894]\n",
      "Mode: Test env_steps 200 total rewards -1515.9913493543863 total energy tensor([[144.6905]])\n",
      "[-0.45471397]\n",
      "Mode: Test env_steps 200 total rewards -1577.2756012380123 total energy tensor([[166.0901]])\n",
      "[0.6268348]\n",
      "Mode: Test env_steps 200 total rewards -1913.2778205871582 total energy tensor([[1.8658]])\n",
      "[0.9688026]\n",
      "Mode: Test env_steps 200 total rewards -1491.8189459443092 total energy tensor([[130.5346]])\n",
      "[-0.37145162]\n",
      "Mode: Test env_steps 200 total rewards -1611.1896731853485 total energy tensor([[168.9973]])\n",
      "10000 -1568.9698884695767\n",
      "[0.10275613]\n",
      "Mode: Train env_steps 200 total rewards -1615.892320394516 total energy tensor([[170.1739]])\n",
      "[0.20799516]\n",
      "Mode: Train env_steps 200 total rewards -1548.269158065319 total energy tensor([[148.6100]])\n",
      "[0.32643446]\n",
      "Mode: Train env_steps 200 total rewards -1919.9888772964478 total energy tensor([[1.7384]])\n",
      "[-0.5415978]\n",
      "Mode: Train env_steps 200 total rewards -1932.7666053771973 total energy tensor([[1.2084]])\n",
      "[0.39155266]\n",
      "Mode: Train env_steps 200 total rewards -1876.339388370514 total energy tensor([[2.4032]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:48<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1599921]\n",
      "Mode: Train env_steps 200 total rewards -1633.14479804039 total energy tensor([[180.2180]])\n",
      "[-0.6888604]\n",
      "Mode: Train env_steps 200 total rewards -1848.7131152153015 total energy tensor([[2.3004]])\n",
      "[-0.61524165]\n",
      "Mode: Train env_steps 200 total rewards -1926.1032905578613 total energy tensor([[1.4972]])\n",
      "[-0.28260043]\n",
      "Mode: Train env_steps 200 total rewards -1555.1142466068268 total energy tensor([[156.3283]])\n",
      "[0.1762983]\n",
      "Mode: Train env_steps 200 total rewards -1938.098424911499 total energy tensor([[0.9474]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32413808]\n",
      "Mode: Train env_steps 200 total rewards -1633.885877341032 total energy tensor([[182.8791]])\n",
      "[0.43044457]\n",
      "Mode: Train env_steps 200 total rewards -1645.906468808651 total energy tensor([[183.3582]])\n",
      "[0.16840471]\n",
      "Mode: Train env_steps 200 total rewards -1919.4635977745056 total energy tensor([[1.5437]])\n",
      "[0.99078465]\n",
      "Mode: Train env_steps 200 total rewards -1890.980372428894 total energy tensor([[2.2630]])\n",
      "[-0.6927419]\n",
      "Mode: Train env_steps 200 total rewards -1838.686912536621 total energy tensor([[3.0092]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8044674]\n",
      "Mode: Train env_steps 200 total rewards -1782.8735976219177 total energy tensor([[2.9067]])\n",
      "[-0.42788523]\n",
      "Mode: Train env_steps 200 total rewards -1516.7631882429123 total energy tensor([[142.3286]])\n",
      "[0.6187882]\n",
      "Mode: Train env_steps 200 total rewards -1560.5180988311768 total energy tensor([[163.9623]])\n",
      "[0.62228]\n",
      "Mode: Train env_steps 200 total rewards -1621.3189475536346 total energy tensor([[184.8376]])\n",
      "[0.4871931]\n",
      "Mode: Train env_steps 200 total rewards -1531.0628660917282 total energy tensor([[149.9875]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6548461]\n",
      "Mode: Train env_steps 200 total rewards -1401.1626613512635 total energy tensor([[140.8096]])\n",
      "[-0.7380303]\n",
      "Mode: Train env_steps 200 total rewards -1946.8271017074585 total energy tensor([[0.6389]])\n",
      "[-0.13667013]\n",
      "Mode: Train env_steps 200 total rewards -1580.3786436319351 total energy tensor([[171.2686]])\n",
      "[0.4475782]\n",
      "Mode: Train env_steps 200 total rewards -1579.7315204441547 total energy tensor([[181.3769]])\n",
      "[-0.7010048]\n",
      "Mode: Train env_steps 200 total rewards -1939.3147706985474 total energy tensor([[1.1237]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:18<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7755755]\n",
      "Mode: Test env_steps 200 total rewards -1583.850512623787 total energy tensor([[170.8460]])\n",
      "[0.4115116]\n",
      "Mode: Test env_steps 200 total rewards -1221.2874257480726 total energy tensor([[105.6162]])\n",
      "[-0.75288486]\n",
      "Mode: Test env_steps 200 total rewards -1579.1257704626769 total energy tensor([[182.1161]])\n",
      "[0.18034057]\n",
      "Mode: Test env_steps 200 total rewards -1568.1845009326935 total energy tensor([[168.7669]])\n",
      "[0.8122652]\n",
      "Mode: Test env_steps 200 total rewards -1912.386471748352 total energy tensor([[1.9767]])\n",
      "[-0.19943881]\n",
      "Mode: Test env_steps 200 total rewards -1134.3799818726256 total energy tensor([[99.1288]])\n",
      "[-0.28473204]\n",
      "Mode: Test env_steps 200 total rewards -1524.179848909378 total energy tensor([[160.4480]])\n",
      "[-0.8504278]\n",
      "Mode: Test env_steps 200 total rewards -1924.3445177078247 total energy tensor([[1.8270]])\n",
      "[-0.9131769]\n",
      "Mode: Test env_steps 200 total rewards -1921.9585580825806 total energy tensor([[1.9523]])\n",
      "[-0.27747452]\n",
      "Mode: Test env_steps 200 total rewards -1821.253562450409 total energy tensor([[3.2555]])\n",
      "15000 -1619.09511505384\n",
      "[-0.74730426]\n",
      "Mode: Train env_steps 200 total rewards -1402.2443251907825 total energy tensor([[144.8895]])\n",
      "[-0.32772848]\n",
      "Mode: Train env_steps 200 total rewards -1639.1073719859123 total energy tensor([[190.5601]])\n",
      "[-0.9812289]\n",
      "Mode: Train env_steps 200 total rewards -1477.1929500252008 total energy tensor([[157.3287]])\n",
      "[0.90568405]\n",
      "Mode: Train env_steps 200 total rewards -1531.863375544548 total energy tensor([[152.7464]])\n",
      "[-0.20725617]\n",
      "Mode: Train env_steps 200 total rewards -1632.7748641967773 total energy tensor([[189.5487]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70642376]\n",
      "Mode: Train env_steps 200 total rewards -1580.261923134327 total energy tensor([[180.5030]])\n",
      "[-0.38362578]\n",
      "Mode: Train env_steps 200 total rewards -1265.0422469377518 total energy tensor([[83.9454]])\n",
      "[-0.65218663]\n",
      "Mode: Train env_steps 200 total rewards -1563.4446083307266 total energy tensor([[167.5109]])\n",
      "[-0.909472]\n",
      "Mode: Train env_steps 200 total rewards -1813.333912372589 total energy tensor([[3.5617]])\n",
      "[-0.62521166]\n",
      "Mode: Train env_steps 200 total rewards -1602.8814917057753 total energy tensor([[187.6884]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:13<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63980085]\n",
      "Mode: Train env_steps 200 total rewards -1929.8248805999756 total energy tensor([[1.3611]])\n",
      "[-0.41099325]\n",
      "Mode: Train env_steps 200 total rewards -1420.258905224502 total energy tensor([[149.0931]])\n",
      "[0.79847807]\n",
      "Mode: Train env_steps 200 total rewards -1762.5496401786804 total energy tensor([[3.6167]])\n",
      "[0.7797261]\n",
      "Mode: Train env_steps 200 total rewards -1521.9018638134003 total energy tensor([[150.9355]])\n",
      "[-0.0791804]\n",
      "Mode: Train env_steps 200 total rewards -1488.4741319417953 total energy tensor([[129.9081]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13292982]\n",
      "Mode: Train env_steps 200 total rewards -1475.902972459793 total energy tensor([[134.6350]])\n",
      "[-0.6884185]\n",
      "Mode: Train env_steps 200 total rewards -1633.0113563537598 total energy tensor([[189.8305]])\n",
      "[-0.61551696]\n",
      "Mode: Train env_steps 200 total rewards -1462.1281624436378 total energy tensor([[126.4449]])\n",
      "[-0.1768803]\n",
      "Mode: Train env_steps 200 total rewards -1543.240952834487 total energy tensor([[162.6967]])\n",
      "[0.26363453]\n",
      "Mode: Train env_steps 200 total rewards -1526.7065216004848 total energy tensor([[157.6806]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4762924]\n",
      "Mode: Train env_steps 200 total rewards -1623.2258843183517 total energy tensor([[187.1408]])\n",
      "[0.99629647]\n",
      "Mode: Train env_steps 200 total rewards -1555.9145169258118 total energy tensor([[164.2038]])\n",
      "[0.12906049]\n",
      "Mode: Train env_steps 200 total rewards -1868.6943001747131 total energy tensor([[2.7933]])\n",
      "[0.64094114]\n",
      "Mode: Train env_steps 200 total rewards -1558.6559108784422 total energy tensor([[177.5862]])\n",
      "[-0.6625711]\n",
      "Mode: Train env_steps 200 total rewards -1693.5302453041077 total energy tensor([[4.9267]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6380241]\n",
      "Mode: Test env_steps 200 total rewards -1956.2669343948364 total energy tensor([[0.5630]])\n",
      "[-0.36989704]\n",
      "Mode: Test env_steps 200 total rewards -1564.3688442111015 total energy tensor([[165.5622]])\n",
      "[0.778222]\n",
      "Mode: Test env_steps 200 total rewards -1294.252350345254 total energy tensor([[82.4476]])\n",
      "[0.536951]\n",
      "Mode: Test env_steps 200 total rewards -1560.7814950942993 total energy tensor([[165.5268]])\n",
      "[0.85153985]\n",
      "Mode: Test env_steps 200 total rewards -1584.728413619101 total energy tensor([[180.8173]])\n",
      "[-0.4528105]\n",
      "Mode: Test env_steps 200 total rewards -1791.3184142112732 total energy tensor([[4.3643]])\n",
      "[0.02103655]\n",
      "Mode: Test env_steps 200 total rewards -1622.4556182920933 total energy tensor([[187.3471]])\n",
      "[0.82927877]\n",
      "Mode: Test env_steps 200 total rewards -1657.6632115840912 total energy tensor([[189.1216]])\n",
      "[-0.26068917]\n",
      "Mode: Test env_steps 200 total rewards -1185.4368878901005 total energy tensor([[20.9354]])\n",
      "[-0.7090724]\n",
      "Mode: Test env_steps 200 total rewards -1432.1342865228653 total energy tensor([[121.4312]])\n",
      "20000 -1564.9406456165016\n",
      "[-0.22766142]\n",
      "Mode: Train env_steps 200 total rewards -1889.17702627182 total energy tensor([[2.1785]])\n",
      "[-0.36365488]\n",
      "Mode: Train env_steps 200 total rewards -1475.7045632377267 total energy tensor([[169.4404]])\n",
      "[-0.6929335]\n",
      "Mode: Train env_steps 200 total rewards -1848.0935068130493 total energy tensor([[3.0363]])\n",
      "[-0.63605213]\n",
      "Mode: Train env_steps 200 total rewards -1298.773910999298 total energy tensor([[82.9526]])\n",
      "[0.63166773]\n",
      "Mode: Train env_steps 200 total rewards -1559.3528116941452 total energy tensor([[164.2994]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22729349]\n",
      "Mode: Train env_steps 200 total rewards -1836.8441700935364 total energy tensor([[4.8747]])\n",
      "[0.45464116]\n",
      "Mode: Train env_steps 200 total rewards -1450.4538716003299 total energy tensor([[154.2228]])\n",
      "[-0.9383594]\n",
      "Mode: Train env_steps 200 total rewards -1205.8366934508085 total energy tensor([[47.2379]])\n",
      "[-0.23815139]\n",
      "Mode: Train env_steps 200 total rewards -1338.94384470582 total energy tensor([[104.3900]])\n",
      "[-0.76947427]\n",
      "Mode: Train env_steps 200 total rewards -1949.2370471954346 total energy tensor([[0.5583]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7039796]\n",
      "Mode: Train env_steps 200 total rewards -1859.483868598938 total energy tensor([[3.2718]])\n",
      "[-0.98048836]\n",
      "Mode: Train env_steps 200 total rewards -1552.5771242678165 total energy tensor([[164.8958]])\n",
      "[-0.6726894]\n",
      "Mode: Train env_steps 200 total rewards -1918.0378684997559 total energy tensor([[2.1549]])\n",
      "[-0.91917175]\n",
      "Mode: Train env_steps 200 total rewards -1874.0962281227112 total energy tensor([[2.9727]])\n",
      "[0.32118738]\n",
      "Mode: Train env_steps 200 total rewards -1534.6853347420692 total energy tensor([[161.3980]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:18<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9200177]\n",
      "Mode: Train env_steps 200 total rewards -1931.9645223617554 total energy tensor([[1.3840]])\n",
      "[-0.48450485]\n",
      "Mode: Train env_steps 200 total rewards -1624.0453878641129 total energy tensor([[186.1614]])\n",
      "[0.61567676]\n",
      "Mode: Train env_steps 200 total rewards -1513.319035757333 total energy tensor([[161.4876]])\n",
      "[-0.5664824]\n",
      "Mode: Train env_steps 200 total rewards -1900.7502689361572 total energy tensor([[2.3817]])\n",
      "[-0.6554988]\n",
      "Mode: Train env_steps 200 total rewards -1925.666184425354 total energy tensor([[1.8183]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:20<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24706006]\n",
      "Mode: Train env_steps 200 total rewards -1648.9352271556854 total energy tensor([[190.8652]])\n",
      "[0.66996366]\n",
      "Mode: Train env_steps 200 total rewards -1947.4450283050537 total energy tensor([[1.0803]])\n",
      "[-0.27475578]\n",
      "Mode: Train env_steps 200 total rewards -1652.222731411457 total energy tensor([[191.3376]])\n",
      "[0.30260766]\n",
      "Mode: Train env_steps 200 total rewards -1611.1025849878788 total energy tensor([[188.0383]])\n",
      "[0.34630948]\n",
      "Mode: Train env_steps 200 total rewards -1628.6147825717926 total energy tensor([[189.7363]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16545616]\n",
      "Mode: Test env_steps 200 total rewards -1062.615165702533 total energy tensor([[74.5741]])\n",
      "[0.8755364]\n",
      "Mode: Test env_steps 200 total rewards -1339.0814244151115 total energy tensor([[114.6412]])\n",
      "[0.96283424]\n",
      "Mode: Test env_steps 200 total rewards -1799.3880281448364 total energy tensor([[5.9255]])\n",
      "[-0.5272495]\n",
      "Mode: Test env_steps 200 total rewards -1870.2569093704224 total energy tensor([[4.3140]])\n",
      "[-0.89746386]\n",
      "Mode: Test env_steps 200 total rewards -1647.4229617714882 total energy tensor([[191.1246]])\n",
      "[0.53423387]\n",
      "Mode: Test env_steps 200 total rewards -1814.1247220039368 total energy tensor([[5.4035]])\n",
      "[-0.36603174]\n",
      "Mode: Test env_steps 200 total rewards -1855.7251420021057 total energy tensor([[5.0196]])\n",
      "[-0.23064801]\n",
      "Mode: Test env_steps 200 total rewards -1641.6919810771942 total energy tensor([[190.4475]])\n",
      "[-0.1160769]\n",
      "Mode: Test env_steps 200 total rewards -1835.3629174232483 total energy tensor([[5.9821]])\n",
      "[-0.9464604]\n",
      "Mode: Test env_steps 200 total rewards -1888.7200379371643 total energy tensor([[2.3896]])\n",
      "25000 -1675.438928984804\n",
      "[0.14425701]\n",
      "Mode: Train env_steps 200 total rewards -1947.060486793518 total energy tensor([[0.8653]])\n",
      "[0.12442216]\n",
      "Mode: Train env_steps 200 total rewards -1852.657730102539 total energy tensor([[3.4215]])\n",
      "[0.14362141]\n",
      "Mode: Train env_steps 200 total rewards -1818.911213874817 total energy tensor([[5.3032]])\n",
      "[-0.00827419]\n",
      "Mode: Train env_steps 200 total rewards -1772.584979057312 total energy tensor([[10.1276]])\n",
      "[-0.14761545]\n",
      "Mode: Train env_steps 200 total rewards -1607.2850486040115 total energy tensor([[188.0052]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8799375]\n",
      "Mode: Train env_steps 200 total rewards -1637.9219407439232 total energy tensor([[190.1309]])\n",
      "[0.57347476]\n",
      "Mode: Train env_steps 200 total rewards -1659.9842394590378 total energy tensor([[191.2721]])\n",
      "[0.5607218]\n",
      "Mode: Train env_steps 200 total rewards -1135.4844479858875 total energy tensor([[51.0188]])\n",
      "[-0.8271978]\n",
      "Mode: Train env_steps 200 total rewards -1165.7514541344717 total energy tensor([[91.1422]])\n",
      "[-0.5216405]\n",
      "Mode: Train env_steps 200 total rewards -1938.1966609954834 total energy tensor([[1.3292]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3053096]\n",
      "Mode: Train env_steps 200 total rewards -1888.2909588813782 total energy tensor([[2.5855]])\n",
      "[0.9674701]\n",
      "Mode: Train env_steps 200 total rewards -1622.394239783287 total energy tensor([[187.5777]])\n",
      "[-0.41176906]\n",
      "Mode: Train env_steps 200 total rewards -1946.5116176605225 total energy tensor([[1.0062]])\n",
      "[-0.4179811]\n",
      "Mode: Train env_steps 200 total rewards -1633.4796619415283 total energy tensor([[190.4261]])\n",
      "[-0.4677181]\n",
      "Mode: Train env_steps 200 total rewards -1368.8728082180023 total energy tensor([[23.6058]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:13<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3864167]\n",
      "Mode: Train env_steps 200 total rewards -1946.841236114502 total energy tensor([[0.6581]])\n",
      "[0.37941763]\n",
      "Mode: Train env_steps 200 total rewards -1945.2846193313599 total energy tensor([[1.0908]])\n",
      "[-0.7855552]\n",
      "Mode: Train env_steps 200 total rewards -1625.236055135727 total energy tensor([[188.3865]])\n",
      "[0.4721376]\n",
      "Mode: Train env_steps 200 total rewards -1854.2765855789185 total energy tensor([[6.6304]])\n",
      "[0.20459862]\n",
      "Mode: Train env_steps 200 total rewards -1401.8805564939976 total energy tensor([[141.4057]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:05<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8268382]\n",
      "Mode: Train env_steps 200 total rewards -1851.8244032859802 total energy tensor([[3.9174]])\n",
      "[0.3122738]\n",
      "Mode: Train env_steps 200 total rewards -1867.585491657257 total energy tensor([[3.1110]])\n",
      "[-0.9366995]\n",
      "Mode: Train env_steps 200 total rewards -1650.426581978798 total energy tensor([[191.6896]])\n",
      "[-0.07240005]\n",
      "Mode: Train env_steps 200 total rewards -1648.9161781668663 total energy tensor([[190.9430]])\n",
      "[-0.22405425]\n",
      "Mode: Train env_steps 200 total rewards -1606.114557981491 total energy tensor([[187.1293]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:03<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09119865]\n",
      "Mode: Test env_steps 200 total rewards -1862.539086818695 total energy tensor([[3.2006]])\n",
      "[0.70383155]\n",
      "Mode: Test env_steps 200 total rewards -1609.7961013615131 total energy tensor([[188.4602]])\n",
      "[0.14944714]\n",
      "Mode: Test env_steps 200 total rewards -1667.314734339714 total energy tensor([[192.5339]])\n",
      "[0.16318478]\n",
      "Mode: Test env_steps 200 total rewards -1731.7203001976013 total energy tensor([[12.0179]])\n",
      "[0.38415682]\n",
      "Mode: Test env_steps 200 total rewards -1590.2090647220612 total energy tensor([[181.1318]])\n",
      "[-0.99951184]\n",
      "Mode: Test env_steps 200 total rewards -1817.8987503051758 total energy tensor([[5.6888]])\n",
      "[-0.11329564]\n",
      "Mode: Test env_steps 200 total rewards -1656.6020753383636 total energy tensor([[20.7113]])\n",
      "[-0.32362702]\n",
      "Mode: Test env_steps 200 total rewards -1730.8598235845566 total energy tensor([[16.9991]])\n",
      "[0.6062914]\n",
      "Mode: Test env_steps 200 total rewards -1815.3972759246826 total energy tensor([[8.8554]])\n",
      "[0.14368069]\n",
      "Mode: Test env_steps 200 total rewards -1901.1961331367493 total energy tensor([[2.7115]])\n",
      "30000 -1738.3533345729113\n",
      "[0.93864435]\n",
      "Mode: Train env_steps 200 total rewards -1936.5836019515991 total energy tensor([[1.4584]])\n",
      "[0.0440227]\n",
      "Mode: Train env_steps 200 total rewards -1926.1882238388062 total energy tensor([[1.9425]])\n",
      "[0.01652334]\n",
      "Mode: Train env_steps 200 total rewards -1734.9099222421646 total energy tensor([[16.6942]])\n",
      "[-0.3174062]\n",
      "Mode: Train env_steps 200 total rewards -1618.1690017580986 total energy tensor([[189.3254]])\n",
      "[0.5469819]\n",
      "Mode: Train env_steps 200 total rewards -1801.3656253814697 total energy tensor([[9.8981]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:03<01:12,  3.03s/it]"
     ]
    }
   ],
   "source": [
    "policy_storage = SeqReplayBuffer(\n",
    "    max_replay_buffer_size=buffer_size,\n",
    "    observation_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    sampled_seq_len=sampled_seq_len,\n",
    "    sample_weight_baseline=0.0,\n",
    ")\n",
    "\n",
    "env_steps = collect_rollouts(\n",
    "    num_rollouts=num_init_rollouts_pool, random_actions=False, train_mode=True\n",
    ")\n",
    "_n_env_steps_total += env_steps\n",
    "\n",
    "# evaluation parameters\n",
    "last_eval_num_iters = 10\n",
    "log_interval = 5\n",
    "eval_num_rollouts = 10\n",
    "learning_curve = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"z\": [],\n",
    "}\n",
    "epoch=0\n",
    "lambda_pat = 0.65\n",
    "\n",
    "while _n_env_steps_total < n_env_steps_total:\n",
    "\n",
    "    env_steps = collect_rollouts(num_rollouts=num_rollouts_per_iter, train_mode=True)\n",
    "    _n_env_steps_total += env_steps\n",
    "\n",
    "    #train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "    factor= lambda_pat **(epoch )\n",
    "    #train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "    train_stats = update(25, lr)\n",
    "    \n",
    "    epoch += 1\n",
    "    current_num_iters = _n_env_steps_total // (\n",
    "        num_rollouts_per_iter * max_trajectory_len\n",
    "    )\n",
    "    if (\n",
    "        current_num_iters != last_eval_num_iters\n",
    "        and current_num_iters % log_interval == 0\n",
    "    ):\n",
    "        last_eval_num_iters = current_num_iters\n",
    "        average_returns, std_returns = collect_rollouts(\n",
    "            num_rollouts=eval_num_rollouts,\n",
    "            train_mode=False,\n",
    "            random_actions=False,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        learning_curve[\"x\"].append(_n_env_steps_total)\n",
    "        learning_curve[\"y\"].append(average_returns)\n",
    "        learning_curve[\"z\"].append(std_returns)\n",
    "        print(_n_env_steps_total, average_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(learning_curve)\n",
    "plt.plot(learning_curve[\"x\"], learning_curve[\"y\"])\n",
    "plt.fill_between(np.array(learning_curve[\"x\"]), np.array(learning_curve[\"y\"])-np.array(learning_curve[\"z\"]), np.array(learning_curve[\"y\"])+np.array(learning_curve[\"z\"]))\n",
    "plt.xlabel(\"env steps\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaning_curve_ncde_64_rk4 = learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timess=torch.linspace(0, 65-1, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('config.txt', 'w')\n",
    "file1.write(str(conf))\n",
    "\n",
    "file1.close()\n",
    "file2 = open('results.txt', 'w')\n",
    "file2.write(str(learning_curve))\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5f49fb63f78fde0f27b95a7c8e14eeaa9af6d816174ff450f7bbbcd21c7c97c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
