{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexander.vasilyev\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\alexander.vasilyev\\pomdp-baselines-main\\utils\\logger.py:9: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import OrderedDict, Set\n",
      "C:\\Users\\alexander.vasilyev\\Anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchkit.pytorch_utils as ptu\n",
    "import torchsde\n",
    "from torch.nn import functional as F\n",
    "import random as rnd\n",
    "import copy as cp\n",
    "# import environments\n",
    "import envs.pomdp\n",
    "import pdb\n",
    "# import recurrent model-free RL (separate architecture)\n",
    "from policies.models.policy_rnn import ModelFreeOffPolicy_Separate_RNN as Policy_RNN\n",
    "from policies.models.policy_rnn_shared import ModelFreeOffPolicy_Shared_RNN as Policy_Shared_RNN\n",
    "from policies.models.policy_mlp import ModelFreeOffPolicy_MLP as Policy_MLP\n",
    "from tqdm import tqdm\n",
    "# import the replay buffer\n",
    "from buffers.seq_replay_buffer_vanilla import SeqReplayBuffer\n",
    "from buffers.simple_replay_buffer import SimpleReplayBuffer \n",
    "from utils import helpers as utl\n",
    "from typing import Sequence\n",
    "from read_ini import read_ini\n",
    "conf =read_ini(\"C:/Users/alexander.vasilyev/pomdp-baselines-main/configfile.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a POMDP environment: Pendulum-V (only observe the velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelFreeOffPolicy_Separate_RNN(\n",
      "  (critic): Critic_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_shortcut_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=2, out_features=48, bias=True)\n",
      "    )\n",
      "    (qf1): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (qf2): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (critic_target): Critic_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_shortcut_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=2, out_features=48, bias=True)\n",
      "    )\n",
      "    (qf1): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (qf2): FlattenMlp(\n",
      "      (fc0): Linear(in_features=120, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (actor): Actor_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (policy): DeterministicPolicy(\n",
      "      (fc0): Linear(in_features=104, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (actor_target): Actor_RNN(\n",
      "    (observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (action_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (reward_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (rnn): NeuralCDE(\n",
      "      (func): CDEFunc(\n",
      "        (linear0): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear1): Linear(in_features=72, out_features=72, bias=True)\n",
      "        (linear2): Linear(in_features=72, out_features=3528, bias=True)\n",
      "      )\n",
      "      (initial): Linear(in_features=49, out_features=72, bias=True)\n",
      "      (readout): Linear(in_features=72, out_features=72, bias=True)\n",
      "    )\n",
      "    (current_observ_embedder): FeatureExtractor(\n",
      "      (fc): Linear(in_features=1, out_features=32, bias=True)\n",
      "    )\n",
      "    (policy): DeterministicPolicy(\n",
      "      (fc0): Linear(in_features=104, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (last_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "total env episodes 2505 total env steps 501000\n"
     ]
    }
   ],
   "source": [
    "cuda_id = 0  # -1 if using cpu\n",
    "ptu.set_gpu_mode(torch.cuda.is_available() and cuda_id >= 0, cuda_id)\n",
    "\n",
    "env = gym.make(conf[\"env_name\"])\n",
    "max_trajectory_len = env._max_episode_steps\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "shared = False\n",
    "markov = False\n",
    "\n",
    "if markov:\n",
    "    agent = Policy_MLP(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=act_dim,\n",
    "        algo_name=conf[\"algo_name\"],\n",
    "        dqn_layers=[128, 128],\n",
    "        policy_layers=[128, 128],\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=5e-3,\n",
    "    ).to(ptu.device)\n",
    "    encoder=\"Nan\"\n",
    "else:\n",
    "    if shared:\n",
    "        agent = Policy_Shared_RNN(\n",
    "            obs_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            encoder=conf[\"encoder\"],\n",
    "            algo_name=conf[\"algo_name\"],\n",
    "            action_embedding_size=int(conf[\"action_embedding_size\"]),\n",
    "            observ_embedding_size=int(conf[\"observ_embedding_size\"]),\n",
    "            reward_embedding_size=int(conf[\"reward_embedding_size\"]),\n",
    "            rnn_hidden_size=int(conf[\"hidden_size\"]),\n",
    "            dqn_layers=[128, 128],\n",
    "            policy_layers=[128, 128],\n",
    "            lr=float(conf[\"lr\"]),\n",
    "            gamma=0.9,\n",
    "            tau=0.005,\n",
    "            embed=True,\n",
    "        ).to(ptu.device)\n",
    "    else: \n",
    "        agent = Policy_RNN(\n",
    "            obs_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            encoder=conf[\"encoder\"],\n",
    "            algo_name=conf[\"algo_name\"],\n",
    "            action_embedding_size=int(conf[\"action_embedding_size\"]),\n",
    "            observ_embedding_size=int(conf[\"observ_embedding_size\"]),\n",
    "            reward_embedding_size=int(conf[\"reward_embedding_size\"]),\n",
    "            rnn_hidden_size=int(conf[\"hidden_size\"]),\n",
    "            dqn_layers=[128, 128],\n",
    "            policy_layers=[128, 128],\n",
    "            lr=float(conf[\"lr\"]),\n",
    "            gamma=0.9,\n",
    "            tau=0.005,\n",
    "            radii=40,\n",
    "            embed=True,\n",
    "            activation = conf[\"activation\"],\n",
    "        ).to(ptu.device)\n",
    "    \n",
    "print(agent)\n",
    "lr=float(conf[\"lr\"])\n",
    "encoder=conf[\"encoder\"]\n",
    "num_updates_per_iter = int(conf[\"num_updates_per_iter\"])  # training frequency\n",
    "sampled_seq_len = int(conf[\"sampled_seq_len\"])  # context length\n",
    "buffer_size = int(float(conf[\"buffer_size\"]))\n",
    "batch_size = int(conf[\"batch_size\"])\n",
    "dropout_rate=float(conf[\"dropout_rate\"])\n",
    "num_iters = int(conf[\"num_iters\"])\n",
    "num_init_rollouts_pool = int(conf[\"num_init_rollouts_pool\"])\n",
    "num_rollouts_per_iter = int(conf[\"num_rollouts_per_iter\"])\n",
    "total_rollouts = num_init_rollouts_pool + num_iters * num_rollouts_per_iter\n",
    "n_env_steps_total = max_trajectory_len * total_rollouts\n",
    "_n_env_steps_total = 0\n",
    "print(\"total env episodes\", total_rollouts, \"total env steps\", n_env_steps_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurent model-free RL agent: separate architecture, `lstm` encoder, `oar` policy input space, `td3` RL algorithm (context length set later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define other training parameters such as context length and training frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define key functions: collect rollouts and policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ncde_row(obs, next_obs, prev_action, action, prev_reward, reward, steps,init):\n",
    "    \n",
    "    if init:\n",
    "        obs_row= obs\n",
    "        rew_row = prev_reward\n",
    "        act_row = prev_action\n",
    "    else:\n",
    "        obs_row=torch.cat((obs, next_obs),0)\n",
    "        rew_row=torch.cat((prev_reward, reward),0)\n",
    "        act_row=torch.cat((prev_action, action),0)\n",
    " \n",
    "    if shared: \n",
    "        obs_row=agent.observ_embedder(obs_row)\n",
    "        rew_row=agent.reward_embedder(rew_row)\n",
    "        act_row=agent.action_embedder(act_row)\n",
    "    else: \n",
    "        obs_row=agent.actor.observ_embedder(obs_row)\n",
    "        rew_row=agent.actor.reward_embedder(rew_row)\n",
    "        act_row=agent.actor.action_embedder(act_row)\n",
    "    \n",
    "    if init:\n",
    "        time_tensor=torch.tensor([[steps]]).to(ptu.device)\n",
    "    else:\n",
    "        time_tensor=torch.tensor([[steps],[steps+1]]).to(ptu.device)\n",
    "\n",
    "    ncde_row=torch.cat((time_tensor,act_row,obs_row,rew_row),1)\n",
    "    ncde_row=ncde_row[None,:]\n",
    "    \n",
    "    return ncde_row\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    num_rollouts, random_actions=False, deterministic=True, train_mode=True\n",
    "):\n",
    "    \"\"\"collect num_rollouts of trajectories in task and save into policy buffer\n",
    "    :param\n",
    "        random_actions: whether to use policy to sample actions, or randomly sample action space\n",
    "        deterministic: deterministic action selection?\n",
    "        train_mode: whether to train (stored to buffer) or test\n",
    "    \"\"\"\n",
    "    if not train_mode:\n",
    "        assert random_actions == False and deterministic == True\n",
    "\n",
    "    total_steps = 0\n",
    "    total_rewards = 0.0\n",
    "    trewards =[]\n",
    "    for idx in range(num_rollouts):\n",
    "        steps = 0\n",
    "        rewards = 0.0\n",
    "        energy = 0.0\n",
    "        print(env.reset())\n",
    "        obs = ptu.from_numpy(env.reset())\n",
    "        obs = obs.reshape(1, obs.shape[-1])\n",
    "        done_rollout = False\n",
    "        init=True\n",
    "        # get hidden state at timestep=0, None for mlp\n",
    "        \n",
    "        if not markov:\n",
    "            action, reward, internal_state = agent.get_initial_info()\n",
    "\n",
    "            if encoder == \"ncde\":\n",
    "                internal_state= None\n",
    "                ncde_row= create_ncde_row(obs, obs, action, action, reward, reward, steps,init)\n",
    "                prev_action= action.clone()\n",
    "                prev_reward= reward.clone()\n",
    "                next_obs= obs.clone()\n",
    "        \n",
    "        \n",
    "        if train_mode:\n",
    "            # temporary storage\n",
    "            obs_list, act_list, rew_list, next_obs_list, term_list = (\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "                           \n",
    "\n",
    "        while not done_rollout:\n",
    "            if markov: \n",
    "                action = agent.act(obs=obs, deterministic=deterministic)[0]\n",
    "            else:\n",
    "                if encoder == \"ncde\":\n",
    "                    (action,_,_,_), internal_state= agent.ncde_act(ncde_row=ncde_row, prev_internal_state=internal_state, obs=obs,  deterministic=deterministic)\n",
    "                else:\n",
    "                    (action, _, _, _), internal_state = agent.act(\n",
    "                        prev_internal_state=internal_state,\n",
    "                        prev_action=action,\n",
    "                        reward=reward,\n",
    "                        obs=obs,\n",
    "                        deterministic=deterministic,\n",
    "                    )\n",
    "            # observe reward and next obs (B=1, dim)\n",
    "            #pdb.set_trace()\n",
    "        \n",
    "            #print(torch.norm(internal_state))\n",
    "            next_obs, reward, done, info = utl.env_step(env, action.squeeze(dim=0))\n",
    "            done_rollout = False if ptu.get_numpy(done[0][0]) == 0.0 else True\n",
    "            init=False\n",
    "            \n",
    "            if not markov:\n",
    "                if encoder == \"ncde\":\n",
    "   \n",
    "                    ncde_row= create_ncde_row(obs, next_obs, prev_action, action, prev_reward, reward, steps,init)\n",
    "            \n",
    "            #switch on/off dropouts\n",
    "            #drop_trigger=rnd.uniform(0,1)\n",
    "            #if drop_trigger<dropout_rate:\n",
    "            #    next_obs=cp.deepcopy(obs)\n",
    "            # update statistics\n",
    "           \n",
    "            rewards += reward.item()\n",
    "            energy += action*action\n",
    "           \n",
    "            # early stopping env: such as rmdp, pomdp, generalize tasks. term ignores timeout\n",
    "            term = (\n",
    "                False\n",
    "                if \"TimeLimit.truncated\" in info or steps >= max_trajectory_len\n",
    "                else done_rollout\n",
    "            )\n",
    "\n",
    "            if train_mode:\n",
    "                # append tensors to temporary storage\n",
    "                obs_list.append(obs)  # (1, dim)\n",
    "                act_list.append(action)  # (1, dim)\n",
    "                rew_list.append(reward)  # (1, dim)\n",
    "                term_list.append(term)  # bool\n",
    "                next_obs_list.append(next_obs)  # (1, dim)\n",
    "            steps += 1\n",
    "            # set: obs <- next_obs\n",
    "            obs = next_obs.clone()\n",
    "            prev_reward= reward.clone()\n",
    "            prev_action= action.clone()\n",
    "        if train_mode:\n",
    "            # add collected sequence to buffer\n",
    "            policy_storage.add_episode(\n",
    "                observations=ptu.get_numpy(torch.cat(obs_list, dim=0)),  # (L, dim)\n",
    "                actions=ptu.get_numpy(torch.cat(act_list, dim=0)),  # (L, dim)\n",
    "                rewards=ptu.get_numpy(torch.cat(rew_list, dim=0)),  # (L, dim)\n",
    "                terminals=np.array(term_list).reshape(-1, 1),  # (L, 1)\n",
    "                next_observations=ptu.get_numpy(\n",
    "                    torch.cat(next_obs_list, dim=0)\n",
    "                ),  # (L, dim)\n",
    "            )\n",
    "        print(\n",
    "            \"Mode:\",\n",
    "            \"Train\" if train_mode else \"Test\",\n",
    "            \"env_steps\",\n",
    "            steps,\n",
    "            \"total rewards\",\n",
    "            rewards,\n",
    "            \"total energy\",\n",
    "            energy,\n",
    "        )\n",
    "        total_steps += steps\n",
    "        total_rewards += rewards\n",
    "        trewards.append(rewards)\n",
    "    if train_mode:\n",
    "        return total_steps\n",
    "    else:\n",
    "        return total_rewards / num_rollouts, np.std(trewards)\n",
    "\n",
    "\n",
    "def update(num_updates, factor):\n",
    "    rl_losses_agg = {}\n",
    "    # print(num_updates)\n",
    "    for update in tqdm(range(num_updates), leave=True):\n",
    "        # sample random RL batch: in transitions\n",
    "        batch = ptu.np_to_pytorch_batch(policy_storage.random_episodes(batch_size))\n",
    "        # RL update\n",
    "        \n",
    "        rl_losses = agent.update(batch, factor)\n",
    "\n",
    "        for k, v in rl_losses.items():\n",
    "            if update == 0:  # first iterate - create list\n",
    "                rl_losses_agg[k] = [v]\n",
    "            else:  # append values\n",
    "                rl_losses_agg[k].append(v)\n",
    "    # statistics\n",
    "    for k in rl_losses_agg:\n",
    "        rl_losses_agg[k] = np.mean(rl_losses_agg[k])\n",
    "    return rl_losses_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the agent: only costs < 20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer RAM usage: 0.02 GB\n",
      "[-0.18286392]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexander.vasilyev\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: Train env_steps 200 total rewards -627.1723878339399 total energy tensor([[0.0009]])\n",
      "[-0.86359847]\n",
      "Mode: Train env_steps 200 total rewards -1465.099901676178 total energy tensor([[0.0006]])\n",
      "[0.74585605]\n",
      "Mode: Train env_steps 200 total rewards -968.4575101137161 total energy tensor([[0.0010]])\n",
      "[-0.8119963]\n",
      "Mode: Train env_steps 200 total rewards -1064.254163146019 total energy tensor([[0.0011]])\n",
      "[-0.65336865]\n",
      "Mode: Train env_steps 200 total rewards -1254.5659635066986 total energy tensor([[0.0009]])\n",
      "[-0.84177446]\n",
      "Mode: Train env_steps 200 total rewards -1584.4519271850586 total energy tensor([[0.0006]])\n",
      "[0.05887923]\n",
      "Mode: Train env_steps 200 total rewards -628.1147714760154 total energy tensor([[0.0009]])\n",
      "[-0.29377258]\n",
      "Mode: Train env_steps 200 total rewards -737.794536806643 total energy tensor([[0.0010]])\n",
      "[-0.8305148]\n",
      "Mode: Train env_steps 200 total rewards -1557.0814833641052 total energy tensor([[0.0006]])\n",
      "[0.61149436]\n",
      "Mode: Train env_steps 200 total rewards -1171.859337568283 total energy tensor([[0.0009]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\alexander.vasilyev\\pomdp-baselines-main\\torchkit\\pytorch_utils.py:73: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if v.dtype == np.bool:\n",
      "100%|██████████| 25/25 [01:02<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6731633]\n",
      "Mode: Train env_steps 200 total rewards -1343.8932387828827 total energy tensor([[31.7528]])\n",
      "[-0.27208433]\n",
      "Mode: Train env_steps 200 total rewards -1266.3245940208435 total energy tensor([[32.4656]])\n",
      "[0.5807534]\n",
      "Mode: Train env_steps 200 total rewards -1699.2578616142273 total energy tensor([[19.7072]])\n",
      "[-0.04527098]\n",
      "Mode: Train env_steps 200 total rewards -1307.411400437355 total energy tensor([[31.1604]])\n",
      "[-0.97157735]\n",
      "Mode: Train env_steps 200 total rewards -1351.332904547453 total energy tensor([[32.0476]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:07<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.48463875]\n",
      "Mode: Train env_steps 200 total rewards -1013.9208480650559 total energy tensor([[3.8427]])\n",
      "[0.5292008]\n",
      "Mode: Train env_steps 200 total rewards -905.1960357353091 total energy tensor([[2.5171]])\n",
      "[-0.27751896]\n",
      "Mode: Train env_steps 200 total rewards -1270.2195472717285 total energy tensor([[2.2214]])\n",
      "[0.39900678]\n",
      "Mode: Train env_steps 200 total rewards -1548.196283340454 total energy tensor([[1.6130]])\n",
      "[0.55401415]\n",
      "Mode: Train env_steps 200 total rewards -1056.3510165549815 total energy tensor([[4.3005]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:14<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8541971]\n",
      "Mode: Train env_steps 200 total rewards -1039.1114730238914 total energy tensor([[6.0354]])\n",
      "[0.41087466]\n",
      "Mode: Train env_steps 200 total rewards -1600.6446633338928 total energy tensor([[2.6838]])\n",
      "[0.76770645]\n",
      "Mode: Train env_steps 200 total rewards -959.2560813948512 total energy tensor([[5.9694]])\n",
      "[0.56193846]\n",
      "Mode: Train env_steps 200 total rewards -1045.8042680621147 total energy tensor([[6.0185]])\n",
      "[-0.4370459]\n",
      "Mode: Train env_steps 200 total rewards -1092.2935677319765 total energy tensor([[6.8191]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:10<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6180618]\n",
      "Mode: Test env_steps 200 total rewards -1662.0854816436768 total energy tensor([[14.0714]])\n",
      "[-0.8945591]\n",
      "Mode: Test env_steps 200 total rewards -1202.432815656066 total energy tensor([[18.3976]])\n",
      "[0.890028]\n",
      "Mode: Test env_steps 200 total rewards -1941.72012424469 total energy tensor([[0.5264]])\n",
      "[0.80951387]\n",
      "Mode: Test env_steps 200 total rewards -1864.3609809875488 total energy tensor([[5.9019]])\n",
      "[0.42710203]\n",
      "Mode: Test env_steps 200 total rewards -1746.466650724411 total energy tensor([[13.6720]])\n",
      "[0.74520326]\n",
      "Mode: Test env_steps 200 total rewards -1943.074589729309 total energy tensor([[0.6629]])\n",
      "[-0.25170442]\n",
      "Mode: Test env_steps 200 total rewards -1814.7647194862366 total energy tensor([[6.0244]])\n",
      "[0.27936238]\n",
      "Mode: Test env_steps 200 total rewards -1629.9115783572197 total energy tensor([[21.4080]])\n",
      "[-0.00369019]\n",
      "Mode: Test env_steps 200 total rewards -1777.9158170223236 total energy tensor([[11.5687]])\n",
      "[0.16060762]\n",
      "Mode: Test env_steps 200 total rewards -1491.5958019224927 total energy tensor([[23.8369]])\n",
      "5000 -1707.4328559773974\n",
      "[-0.02452885]\n",
      "Mode: Train env_steps 200 total rewards -1716.6345075368881 total energy tensor([[15.9769]])\n",
      "[0.07957311]\n",
      "Mode: Train env_steps 200 total rewards -1781.354884147644 total energy tensor([[7.6416]])\n",
      "[0.19677001]\n",
      "Mode: Train env_steps 200 total rewards -1744.4727334976196 total energy tensor([[13.7834]])\n",
      "[-0.22868241]\n",
      "Mode: Train env_steps 200 total rewards -1860.7293000221252 total energy tensor([[2.9837]])\n",
      "[-0.63196903]\n",
      "Mode: Train env_steps 200 total rewards -1827.9240417480469 total energy tensor([[4.2675]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:15<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9493371]\n",
      "Mode: Train env_steps 200 total rewards -1725.1589938402176 total energy tensor([[11.8734]])\n",
      "[-0.6375915]\n",
      "Mode: Train env_steps 200 total rewards -1766.9023275375366 total energy tensor([[9.1713]])\n",
      "[0.1520319]\n",
      "Mode: Train env_steps 200 total rewards -1849.0722332000732 total energy tensor([[3.8441]])\n",
      "[-0.89367926]\n",
      "Mode: Train env_steps 200 total rewards -1626.971121788025 total energy tensor([[16.1817]])\n",
      "[0.26634613]\n",
      "Mode: Train env_steps 200 total rewards -1375.0553357303143 total energy tensor([[42.9877]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:09<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77172476]\n",
      "Mode: Train env_steps 200 total rewards -1404.228477358818 total energy tensor([[56.1248]])\n",
      "[-0.14430985]\n",
      "Mode: Train env_steps 200 total rewards -1414.0377424955368 total energy tensor([[56.4431]])\n",
      "[-0.18019481]\n",
      "Mode: Train env_steps 200 total rewards -1420.5737288594246 total energy tensor([[57.7075]])\n",
      "[0.05015166]\n",
      "Mode: Train env_steps 200 total rewards -1876.0046792030334 total energy tensor([[2.0594]])\n",
      "[0.8314296]\n",
      "Mode: Train env_steps 200 total rewards -1534.730565071106 total energy tensor([[3.8204]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:12<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3954042]\n",
      "Mode: Train env_steps 200 total rewards -1383.6435743272305 total energy tensor([[94.4836]])\n",
      "[0.20333406]\n",
      "Mode: Train env_steps 200 total rewards -1355.230950385332 total energy tensor([[81.6843]])\n",
      "[0.8841586]\n",
      "Mode: Train env_steps 200 total rewards -1192.585041904822 total energy tensor([[68.4791]])\n",
      "[0.83319604]\n",
      "Mode: Train env_steps 200 total rewards -1731.3692646026611 total energy tensor([[5.9787]])\n",
      "[0.0804089]\n",
      "Mode: Train env_steps 200 total rewards -908.4952503293753 total energy tensor([[2.1642]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15499073]\n",
      "Mode: Train env_steps 200 total rewards -1607.0602875351906 total energy tensor([[163.1991]])\n",
      "[-0.19695659]\n",
      "Mode: Train env_steps 200 total rewards -1518.6114336028695 total energy tensor([[137.2252]])\n",
      "[0.4013462]\n",
      "Mode: Train env_steps 200 total rewards -1890.2161655426025 total energy tensor([[2.0544]])\n",
      "[-0.5135966]\n",
      "Mode: Train env_steps 200 total rewards -1951.1738548278809 total energy tensor([[0.4269]])\n",
      "[0.1571486]\n",
      "Mode: Train env_steps 200 total rewards -1874.922622680664 total energy tensor([[1.8949]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:32<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9871652]\n",
      "Mode: Test env_steps 200 total rewards -1388.7813239395618 total energy tensor([[108.5133]])\n",
      "[0.48804292]\n",
      "Mode: Test env_steps 200 total rewards -1611.185109257698 total energy tensor([[168.8188]])\n",
      "[0.5315943]\n",
      "Mode: Test env_steps 200 total rewards -1460.1157260090113 total energy tensor([[131.8175]])\n",
      "[-0.8156318]\n",
      "Mode: Test env_steps 200 total rewards -1585.284763097763 total energy tensor([[163.9564]])\n",
      "[0.4804839]\n",
      "Mode: Test env_steps 200 total rewards -1534.7785720825195 total energy tensor([[142.8596]])\n",
      "[0.24370894]\n",
      "Mode: Test env_steps 200 total rewards -1515.9913493543863 total energy tensor([[144.6905]])\n",
      "[-0.45471397]\n",
      "Mode: Test env_steps 200 total rewards -1577.2756012380123 total energy tensor([[166.0901]])\n",
      "[0.6268348]\n",
      "Mode: Test env_steps 200 total rewards -1913.2778205871582 total energy tensor([[1.8658]])\n",
      "[0.9688026]\n",
      "Mode: Test env_steps 200 total rewards -1491.8189459443092 total energy tensor([[130.5346]])\n",
      "[-0.37145162]\n",
      "Mode: Test env_steps 200 total rewards -1611.1896731853485 total energy tensor([[168.9973]])\n",
      "10000 -1568.9698884695767\n",
      "[0.10275613]\n",
      "Mode: Train env_steps 200 total rewards -1615.892320394516 total energy tensor([[170.1739]])\n",
      "[0.20799516]\n",
      "Mode: Train env_steps 200 total rewards -1548.269158065319 total energy tensor([[148.6100]])\n",
      "[0.32643446]\n",
      "Mode: Train env_steps 200 total rewards -1919.9888772964478 total energy tensor([[1.7384]])\n",
      "[-0.5415978]\n",
      "Mode: Train env_steps 200 total rewards -1932.7666053771973 total energy tensor([[1.2084]])\n",
      "[0.39155266]\n",
      "Mode: Train env_steps 200 total rewards -1876.339388370514 total energy tensor([[2.4032]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:48<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1599921]\n",
      "Mode: Train env_steps 200 total rewards -1633.14479804039 total energy tensor([[180.2180]])\n",
      "[-0.6888604]\n",
      "Mode: Train env_steps 200 total rewards -1848.7131152153015 total energy tensor([[2.3004]])\n",
      "[-0.61524165]\n",
      "Mode: Train env_steps 200 total rewards -1926.1032905578613 total energy tensor([[1.4972]])\n",
      "[-0.28260043]\n",
      "Mode: Train env_steps 200 total rewards -1555.1142466068268 total energy tensor([[156.3283]])\n",
      "[0.1762983]\n",
      "Mode: Train env_steps 200 total rewards -1938.098424911499 total energy tensor([[0.9474]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32413808]\n",
      "Mode: Train env_steps 200 total rewards -1633.885877341032 total energy tensor([[182.8791]])\n",
      "[0.43044457]\n",
      "Mode: Train env_steps 200 total rewards -1645.906468808651 total energy tensor([[183.3582]])\n",
      "[0.16840471]\n",
      "Mode: Train env_steps 200 total rewards -1919.4635977745056 total energy tensor([[1.5437]])\n",
      "[0.99078465]\n",
      "Mode: Train env_steps 200 total rewards -1890.980372428894 total energy tensor([[2.2630]])\n",
      "[-0.6927419]\n",
      "Mode: Train env_steps 200 total rewards -1838.686912536621 total energy tensor([[3.0092]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8044674]\n",
      "Mode: Train env_steps 200 total rewards -1782.8735976219177 total energy tensor([[2.9067]])\n",
      "[-0.42788523]\n",
      "Mode: Train env_steps 200 total rewards -1516.7631882429123 total energy tensor([[142.3286]])\n",
      "[0.6187882]\n",
      "Mode: Train env_steps 200 total rewards -1560.5180988311768 total energy tensor([[163.9623]])\n",
      "[0.62228]\n",
      "Mode: Train env_steps 200 total rewards -1621.3189475536346 total energy tensor([[184.8376]])\n",
      "[0.4871931]\n",
      "Mode: Train env_steps 200 total rewards -1531.0628660917282 total energy tensor([[149.9875]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6548461]\n",
      "Mode: Train env_steps 200 total rewards -1401.1626613512635 total energy tensor([[140.8096]])\n",
      "[-0.7380303]\n",
      "Mode: Train env_steps 200 total rewards -1946.8271017074585 total energy tensor([[0.6389]])\n",
      "[-0.13667013]\n",
      "Mode: Train env_steps 200 total rewards -1580.3786436319351 total energy tensor([[171.2686]])\n",
      "[0.4475782]\n",
      "Mode: Train env_steps 200 total rewards -1579.7315204441547 total energy tensor([[181.3769]])\n",
      "[-0.7010048]\n",
      "Mode: Train env_steps 200 total rewards -1939.3147706985474 total energy tensor([[1.1237]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:18<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7755755]\n",
      "Mode: Test env_steps 200 total rewards -1583.850512623787 total energy tensor([[170.8460]])\n",
      "[0.4115116]\n",
      "Mode: Test env_steps 200 total rewards -1221.2874257480726 total energy tensor([[105.6162]])\n",
      "[-0.75288486]\n",
      "Mode: Test env_steps 200 total rewards -1579.1257704626769 total energy tensor([[182.1161]])\n",
      "[0.18034057]\n",
      "Mode: Test env_steps 200 total rewards -1568.1845009326935 total energy tensor([[168.7669]])\n",
      "[0.8122652]\n",
      "Mode: Test env_steps 200 total rewards -1912.386471748352 total energy tensor([[1.9767]])\n",
      "[-0.19943881]\n",
      "Mode: Test env_steps 200 total rewards -1134.3799818726256 total energy tensor([[99.1288]])\n",
      "[-0.28473204]\n",
      "Mode: Test env_steps 200 total rewards -1524.179848909378 total energy tensor([[160.4480]])\n",
      "[-0.8504278]\n",
      "Mode: Test env_steps 200 total rewards -1924.3445177078247 total energy tensor([[1.8270]])\n",
      "[-0.9131769]\n",
      "Mode: Test env_steps 200 total rewards -1921.9585580825806 total energy tensor([[1.9523]])\n",
      "[-0.27747452]\n",
      "Mode: Test env_steps 200 total rewards -1821.253562450409 total energy tensor([[3.2555]])\n",
      "15000 -1619.09511505384\n",
      "[-0.74730426]\n",
      "Mode: Train env_steps 200 total rewards -1402.2443251907825 total energy tensor([[144.8895]])\n",
      "[-0.32772848]\n",
      "Mode: Train env_steps 200 total rewards -1639.1073719859123 total energy tensor([[190.5601]])\n",
      "[-0.9812289]\n",
      "Mode: Train env_steps 200 total rewards -1477.1929500252008 total energy tensor([[157.3287]])\n",
      "[0.90568405]\n",
      "Mode: Train env_steps 200 total rewards -1531.863375544548 total energy tensor([[152.7464]])\n",
      "[-0.20725617]\n",
      "Mode: Train env_steps 200 total rewards -1632.7748641967773 total energy tensor([[189.5487]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70642376]\n",
      "Mode: Train env_steps 200 total rewards -1580.261923134327 total energy tensor([[180.5030]])\n",
      "[-0.38362578]\n",
      "Mode: Train env_steps 200 total rewards -1265.0422469377518 total energy tensor([[83.9454]])\n",
      "[-0.65218663]\n",
      "Mode: Train env_steps 200 total rewards -1563.4446083307266 total energy tensor([[167.5109]])\n",
      "[-0.909472]\n",
      "Mode: Train env_steps 200 total rewards -1813.333912372589 total energy tensor([[3.5617]])\n",
      "[-0.62521166]\n",
      "Mode: Train env_steps 200 total rewards -1602.8814917057753 total energy tensor([[187.6884]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:13<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63980085]\n",
      "Mode: Train env_steps 200 total rewards -1929.8248805999756 total energy tensor([[1.3611]])\n",
      "[-0.41099325]\n",
      "Mode: Train env_steps 200 total rewards -1420.258905224502 total energy tensor([[149.0931]])\n",
      "[0.79847807]\n",
      "Mode: Train env_steps 200 total rewards -1762.5496401786804 total energy tensor([[3.6167]])\n",
      "[0.7797261]\n",
      "Mode: Train env_steps 200 total rewards -1521.9018638134003 total energy tensor([[150.9355]])\n",
      "[-0.0791804]\n",
      "Mode: Train env_steps 200 total rewards -1488.4741319417953 total energy tensor([[129.9081]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13292982]\n",
      "Mode: Train env_steps 200 total rewards -1475.902972459793 total energy tensor([[134.6350]])\n",
      "[-0.6884185]\n",
      "Mode: Train env_steps 200 total rewards -1633.0113563537598 total energy tensor([[189.8305]])\n",
      "[-0.61551696]\n",
      "Mode: Train env_steps 200 total rewards -1462.1281624436378 total energy tensor([[126.4449]])\n",
      "[-0.1768803]\n",
      "Mode: Train env_steps 200 total rewards -1543.240952834487 total energy tensor([[162.6967]])\n",
      "[0.26363453]\n",
      "Mode: Train env_steps 200 total rewards -1526.7065216004848 total energy tensor([[157.6806]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4762924]\n",
      "Mode: Train env_steps 200 total rewards -1623.2258843183517 total energy tensor([[187.1408]])\n",
      "[0.99629647]\n",
      "Mode: Train env_steps 200 total rewards -1555.9145169258118 total energy tensor([[164.2038]])\n",
      "[0.12906049]\n",
      "Mode: Train env_steps 200 total rewards -1868.6943001747131 total energy tensor([[2.7933]])\n",
      "[0.64094114]\n",
      "Mode: Train env_steps 200 total rewards -1558.6559108784422 total energy tensor([[177.5862]])\n",
      "[-0.6625711]\n",
      "Mode: Train env_steps 200 total rewards -1693.5302453041077 total energy tensor([[4.9267]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6380241]\n",
      "Mode: Test env_steps 200 total rewards -1956.2669343948364 total energy tensor([[0.5630]])\n",
      "[-0.36989704]\n",
      "Mode: Test env_steps 200 total rewards -1564.3688442111015 total energy tensor([[165.5622]])\n",
      "[0.778222]\n",
      "Mode: Test env_steps 200 total rewards -1294.252350345254 total energy tensor([[82.4476]])\n",
      "[0.536951]\n",
      "Mode: Test env_steps 200 total rewards -1560.7814950942993 total energy tensor([[165.5268]])\n",
      "[0.85153985]\n",
      "Mode: Test env_steps 200 total rewards -1584.728413619101 total energy tensor([[180.8173]])\n",
      "[-0.4528105]\n",
      "Mode: Test env_steps 200 total rewards -1791.3184142112732 total energy tensor([[4.3643]])\n",
      "[0.02103655]\n",
      "Mode: Test env_steps 200 total rewards -1622.4556182920933 total energy tensor([[187.3471]])\n",
      "[0.82927877]\n",
      "Mode: Test env_steps 200 total rewards -1657.6632115840912 total energy tensor([[189.1216]])\n",
      "[-0.26068917]\n",
      "Mode: Test env_steps 200 total rewards -1185.4368878901005 total energy tensor([[20.9354]])\n",
      "[-0.7090724]\n",
      "Mode: Test env_steps 200 total rewards -1432.1342865228653 total energy tensor([[121.4312]])\n",
      "20000 -1564.9406456165016\n",
      "[-0.22766142]\n",
      "Mode: Train env_steps 200 total rewards -1889.17702627182 total energy tensor([[2.1785]])\n",
      "[-0.36365488]\n",
      "Mode: Train env_steps 200 total rewards -1475.7045632377267 total energy tensor([[169.4404]])\n",
      "[-0.6929335]\n",
      "Mode: Train env_steps 200 total rewards -1848.0935068130493 total energy tensor([[3.0363]])\n",
      "[-0.63605213]\n",
      "Mode: Train env_steps 200 total rewards -1298.773910999298 total energy tensor([[82.9526]])\n",
      "[0.63166773]\n",
      "Mode: Train env_steps 200 total rewards -1559.3528116941452 total energy tensor([[164.2994]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22729349]\n",
      "Mode: Train env_steps 200 total rewards -1836.8441700935364 total energy tensor([[4.8747]])\n",
      "[0.45464116]\n",
      "Mode: Train env_steps 200 total rewards -1450.4538716003299 total energy tensor([[154.2228]])\n",
      "[-0.9383594]\n",
      "Mode: Train env_steps 200 total rewards -1205.8366934508085 total energy tensor([[47.2379]])\n",
      "[-0.23815139]\n",
      "Mode: Train env_steps 200 total rewards -1338.94384470582 total energy tensor([[104.3900]])\n",
      "[-0.76947427]\n",
      "Mode: Train env_steps 200 total rewards -1949.2370471954346 total energy tensor([[0.5583]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7039796]\n",
      "Mode: Train env_steps 200 total rewards -1859.483868598938 total energy tensor([[3.2718]])\n",
      "[-0.98048836]\n",
      "Mode: Train env_steps 200 total rewards -1552.5771242678165 total energy tensor([[164.8958]])\n",
      "[-0.6726894]\n",
      "Mode: Train env_steps 200 total rewards -1918.0378684997559 total energy tensor([[2.1549]])\n",
      "[-0.91917175]\n",
      "Mode: Train env_steps 200 total rewards -1874.0962281227112 total energy tensor([[2.9727]])\n",
      "[0.32118738]\n",
      "Mode: Train env_steps 200 total rewards -1534.6853347420692 total energy tensor([[161.3980]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:18<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9200177]\n",
      "Mode: Train env_steps 200 total rewards -1931.9645223617554 total energy tensor([[1.3840]])\n",
      "[-0.48450485]\n",
      "Mode: Train env_steps 200 total rewards -1624.0453878641129 total energy tensor([[186.1614]])\n",
      "[0.61567676]\n",
      "Mode: Train env_steps 200 total rewards -1513.319035757333 total energy tensor([[161.4876]])\n",
      "[-0.5664824]\n",
      "Mode: Train env_steps 200 total rewards -1900.7502689361572 total energy tensor([[2.3817]])\n",
      "[-0.6554988]\n",
      "Mode: Train env_steps 200 total rewards -1925.666184425354 total energy tensor([[1.8183]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:20<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24706006]\n",
      "Mode: Train env_steps 200 total rewards -1648.9352271556854 total energy tensor([[190.8652]])\n",
      "[0.66996366]\n",
      "Mode: Train env_steps 200 total rewards -1947.4450283050537 total energy tensor([[1.0803]])\n",
      "[-0.27475578]\n",
      "Mode: Train env_steps 200 total rewards -1652.222731411457 total energy tensor([[191.3376]])\n",
      "[0.30260766]\n",
      "Mode: Train env_steps 200 total rewards -1611.1025849878788 total energy tensor([[188.0383]])\n",
      "[0.34630948]\n",
      "Mode: Train env_steps 200 total rewards -1628.6147825717926 total energy tensor([[189.7363]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16545616]\n",
      "Mode: Test env_steps 200 total rewards -1062.615165702533 total energy tensor([[74.5741]])\n",
      "[0.8755364]\n",
      "Mode: Test env_steps 200 total rewards -1339.0814244151115 total energy tensor([[114.6412]])\n",
      "[0.96283424]\n",
      "Mode: Test env_steps 200 total rewards -1799.3880281448364 total energy tensor([[5.9255]])\n",
      "[-0.5272495]\n",
      "Mode: Test env_steps 200 total rewards -1870.2569093704224 total energy tensor([[4.3140]])\n",
      "[-0.89746386]\n",
      "Mode: Test env_steps 200 total rewards -1647.4229617714882 total energy tensor([[191.1246]])\n",
      "[0.53423387]\n",
      "Mode: Test env_steps 200 total rewards -1814.1247220039368 total energy tensor([[5.4035]])\n",
      "[-0.36603174]\n",
      "Mode: Test env_steps 200 total rewards -1855.7251420021057 total energy tensor([[5.0196]])\n",
      "[-0.23064801]\n",
      "Mode: Test env_steps 200 total rewards -1641.6919810771942 total energy tensor([[190.4475]])\n",
      "[-0.1160769]\n",
      "Mode: Test env_steps 200 total rewards -1835.3629174232483 total energy tensor([[5.9821]])\n",
      "[-0.9464604]\n",
      "Mode: Test env_steps 200 total rewards -1888.7200379371643 total energy tensor([[2.3896]])\n",
      "25000 -1675.438928984804\n",
      "[0.14425701]\n",
      "Mode: Train env_steps 200 total rewards -1947.060486793518 total energy tensor([[0.8653]])\n",
      "[0.12442216]\n",
      "Mode: Train env_steps 200 total rewards -1852.657730102539 total energy tensor([[3.4215]])\n",
      "[0.14362141]\n",
      "Mode: Train env_steps 200 total rewards -1818.911213874817 total energy tensor([[5.3032]])\n",
      "[-0.00827419]\n",
      "Mode: Train env_steps 200 total rewards -1772.584979057312 total energy tensor([[10.1276]])\n",
      "[-0.14761545]\n",
      "Mode: Train env_steps 200 total rewards -1607.2850486040115 total energy tensor([[188.0052]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8799375]\n",
      "Mode: Train env_steps 200 total rewards -1637.9219407439232 total energy tensor([[190.1309]])\n",
      "[0.57347476]\n",
      "Mode: Train env_steps 200 total rewards -1659.9842394590378 total energy tensor([[191.2721]])\n",
      "[0.5607218]\n",
      "Mode: Train env_steps 200 total rewards -1135.4844479858875 total energy tensor([[51.0188]])\n",
      "[-0.8271978]\n",
      "Mode: Train env_steps 200 total rewards -1165.7514541344717 total energy tensor([[91.1422]])\n",
      "[-0.5216405]\n",
      "Mode: Train env_steps 200 total rewards -1938.1966609954834 total energy tensor([[1.3292]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3053096]\n",
      "Mode: Train env_steps 200 total rewards -1888.2909588813782 total energy tensor([[2.5855]])\n",
      "[0.9674701]\n",
      "Mode: Train env_steps 200 total rewards -1622.394239783287 total energy tensor([[187.5777]])\n",
      "[-0.41176906]\n",
      "Mode: Train env_steps 200 total rewards -1946.5116176605225 total energy tensor([[1.0062]])\n",
      "[-0.4179811]\n",
      "Mode: Train env_steps 200 total rewards -1633.4796619415283 total energy tensor([[190.4261]])\n",
      "[-0.4677181]\n",
      "Mode: Train env_steps 200 total rewards -1368.8728082180023 total energy tensor([[23.6058]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:13<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3864167]\n",
      "Mode: Train env_steps 200 total rewards -1946.841236114502 total energy tensor([[0.6581]])\n",
      "[0.37941763]\n",
      "Mode: Train env_steps 200 total rewards -1945.2846193313599 total energy tensor([[1.0908]])\n",
      "[-0.7855552]\n",
      "Mode: Train env_steps 200 total rewards -1625.236055135727 total energy tensor([[188.3865]])\n",
      "[0.4721376]\n",
      "Mode: Train env_steps 200 total rewards -1854.2765855789185 total energy tensor([[6.6304]])\n",
      "[0.20459862]\n",
      "Mode: Train env_steps 200 total rewards -1401.8805564939976 total energy tensor([[141.4057]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:05<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8268382]\n",
      "Mode: Train env_steps 200 total rewards -1851.8244032859802 total energy tensor([[3.9174]])\n",
      "[0.3122738]\n",
      "Mode: Train env_steps 200 total rewards -1867.585491657257 total energy tensor([[3.1110]])\n",
      "[-0.9366995]\n",
      "Mode: Train env_steps 200 total rewards -1650.426581978798 total energy tensor([[191.6896]])\n",
      "[-0.07240005]\n",
      "Mode: Train env_steps 200 total rewards -1648.9161781668663 total energy tensor([[190.9430]])\n",
      "[-0.22405425]\n",
      "Mode: Train env_steps 200 total rewards -1606.114557981491 total energy tensor([[187.1293]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:03<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09119865]\n",
      "Mode: Test env_steps 200 total rewards -1862.539086818695 total energy tensor([[3.2006]])\n",
      "[0.70383155]\n",
      "Mode: Test env_steps 200 total rewards -1609.7961013615131 total energy tensor([[188.4602]])\n",
      "[0.14944714]\n",
      "Mode: Test env_steps 200 total rewards -1667.314734339714 total energy tensor([[192.5339]])\n",
      "[0.16318478]\n",
      "Mode: Test env_steps 200 total rewards -1731.7203001976013 total energy tensor([[12.0179]])\n",
      "[0.38415682]\n",
      "Mode: Test env_steps 200 total rewards -1590.2090647220612 total energy tensor([[181.1318]])\n",
      "[-0.99951184]\n",
      "Mode: Test env_steps 200 total rewards -1817.8987503051758 total energy tensor([[5.6888]])\n",
      "[-0.11329564]\n",
      "Mode: Test env_steps 200 total rewards -1656.6020753383636 total energy tensor([[20.7113]])\n",
      "[-0.32362702]\n",
      "Mode: Test env_steps 200 total rewards -1730.8598235845566 total energy tensor([[16.9991]])\n",
      "[0.6062914]\n",
      "Mode: Test env_steps 200 total rewards -1815.3972759246826 total energy tensor([[8.8554]])\n",
      "[0.14368069]\n",
      "Mode: Test env_steps 200 total rewards -1901.1961331367493 total energy tensor([[2.7115]])\n",
      "30000 -1738.3533345729113\n",
      "[0.93864435]\n",
      "Mode: Train env_steps 200 total rewards -1936.5836019515991 total energy tensor([[1.4584]])\n",
      "[0.0440227]\n",
      "Mode: Train env_steps 200 total rewards -1926.1882238388062 total energy tensor([[1.9425]])\n",
      "[0.01652334]\n",
      "Mode: Train env_steps 200 total rewards -1734.9099222421646 total energy tensor([[16.6942]])\n",
      "[-0.3174062]\n",
      "Mode: Train env_steps 200 total rewards -1618.1690017580986 total energy tensor([[189.3254]])\n",
      "[0.5469819]\n",
      "Mode: Train env_steps 200 total rewards -1801.3656253814697 total energy tensor([[9.8981]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.54640865]\n",
      "Mode: Train env_steps 200 total rewards -1905.9829287528992 total energy tensor([[2.4944]])\n",
      "[0.16435257]\n",
      "Mode: Train env_steps 200 total rewards -1803.3828558921814 total energy tensor([[7.0364]])\n",
      "[-0.10612082]\n",
      "Mode: Train env_steps 200 total rewards -1739.0047221183777 total energy tensor([[17.2823]])\n",
      "[-0.643085]\n",
      "Mode: Train env_steps 200 total rewards -1898.5365929603577 total energy tensor([[2.5747]])\n",
      "[-0.54642993]\n",
      "Mode: Train env_steps 200 total rewards -1830.4607901573181 total energy tensor([[7.9311]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:09<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88004434]\n",
      "Mode: Train env_steps 200 total rewards -1723.6517538428307 total energy tensor([[17.8669]])\n",
      "[-0.29850534]\n",
      "Mode: Train env_steps 200 total rewards -1882.1584944725037 total energy tensor([[2.6082]])\n",
      "[-0.03609039]\n",
      "Mode: Train env_steps 200 total rewards -1763.0903487205505 total energy tensor([[14.5381]])\n",
      "[0.9985618]\n",
      "Mode: Train env_steps 200 total rewards -1918.60142993927 total energy tensor([[2.1187]])\n",
      "[0.53666717]\n",
      "Mode: Train env_steps 200 total rewards -1566.0231897830963 total energy tensor([[158.7878]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:12<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07771582]\n",
      "Mode: Train env_steps 200 total rewards -1381.9204009100795 total energy tensor([[53.0088]])\n",
      "[-0.61696076]\n",
      "Mode: Train env_steps 200 total rewards -1733.5493824481964 total energy tensor([[12.3221]])\n",
      "[-0.15121719]\n",
      "Mode: Train env_steps 200 total rewards -1362.736045371741 total energy tensor([[51.9303]])\n",
      "[0.7536695]\n",
      "Mode: Train env_steps 200 total rewards -1789.7758421897888 total energy tensor([[12.6767]])\n",
      "[0.02391289]\n",
      "Mode: Train env_steps 200 total rewards -1805.379364490509 total energy tensor([[9.9640]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8539833]\n",
      "Mode: Train env_steps 200 total rewards -1388.733128964901 total energy tensor([[51.9785]])\n",
      "[0.9200382]\n",
      "Mode: Train env_steps 200 total rewards -1637.1885667741299 total energy tensor([[21.9539]])\n",
      "[0.8722265]\n",
      "Mode: Train env_steps 200 total rewards -1871.6549530029297 total energy tensor([[2.7283]])\n",
      "[0.08376437]\n",
      "Mode: Train env_steps 200 total rewards -1872.5861763954163 total energy tensor([[4.9484]])\n",
      "[0.5577852]\n",
      "Mode: Train env_steps 200 total rewards -1814.4485671520233 total energy tensor([[9.4235]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:59<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39811623]\n",
      "Mode: Test env_steps 200 total rewards -1929.911904335022 total energy tensor([[1.6947]])\n",
      "[-0.670191]\n",
      "Mode: Test env_steps 200 total rewards -1274.0403387956321 total energy tensor([[43.7960]])\n",
      "[-0.7091089]\n",
      "Mode: Test env_steps 200 total rewards -1748.2800024747849 total energy tensor([[17.6128]])\n",
      "[0.2626294]\n",
      "Mode: Test env_steps 200 total rewards -1916.8864679336548 total energy tensor([[1.9131]])\n",
      "[-0.6625261]\n",
      "Mode: Test env_steps 200 total rewards -1378.2875380069017 total energy tensor([[46.9791]])\n",
      "[-0.63169736]\n",
      "Mode: Test env_steps 200 total rewards -1937.1730375289917 total energy tensor([[1.1619]])\n",
      "[-0.6177671]\n",
      "Mode: Test env_steps 200 total rewards -1667.7905752658844 total energy tensor([[17.6159]])\n",
      "[0.72285956]\n",
      "Mode: Test env_steps 200 total rewards -1367.7181482985616 total energy tensor([[46.8426]])\n",
      "[0.5165133]\n",
      "Mode: Test env_steps 200 total rewards -1923.3268823623657 total energy tensor([[1.9934]])\n",
      "[0.17629433]\n",
      "Mode: Test env_steps 200 total rewards -1381.5630359649658 total energy tensor([[47.0300]])\n",
      "35000 -1652.4977930966766\n",
      "[0.788387]\n",
      "Mode: Train env_steps 200 total rewards -1834.285885334015 total energy tensor([[4.8882]])\n",
      "[0.7816697]\n",
      "Mode: Train env_steps 200 total rewards -1906.264750957489 total energy tensor([[2.7080]])\n",
      "[-0.78915155]\n",
      "Mode: Train env_steps 200 total rewards -1716.2953906059265 total energy tensor([[14.5086]])\n",
      "[0.87031126]\n",
      "Mode: Train env_steps 200 total rewards -1921.5950775146484 total energy tensor([[1.8556]])\n",
      "[-0.93151695]\n",
      "Mode: Train env_steps 200 total rewards -1915.966624736786 total energy tensor([[2.5201]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:02<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7783583]\n",
      "Mode: Train env_steps 200 total rewards -1382.0202741026878 total energy tensor([[48.7553]])\n",
      "[-0.83982015]\n",
      "Mode: Train env_steps 200 total rewards -1833.3649485111237 total energy tensor([[7.9838]])\n",
      "[0.917532]\n",
      "Mode: Train env_steps 200 total rewards -1381.063308224082 total energy tensor([[49.2090]])\n",
      "[-0.926766]\n",
      "Mode: Train env_steps 200 total rewards -1856.5493531227112 total energy tensor([[6.7890]])\n",
      "[0.45223513]\n",
      "Mode: Train env_steps 200 total rewards -1788.0245325565338 total energy tensor([[7.9478]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:03<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.69651604]\n",
      "Mode: Train env_steps 200 total rewards -1804.321132659912 total energy tensor([[7.5655]])\n",
      "[0.7020163]\n",
      "Mode: Train env_steps 200 total rewards -1931.436939239502 total energy tensor([[1.6461]])\n",
      "[0.48916373]\n",
      "Mode: Train env_steps 200 total rewards -1364.724163889885 total energy tensor([[41.8103]])\n",
      "[-0.3392886]\n",
      "Mode: Train env_steps 200 total rewards -1698.2363207936287 total energy tensor([[20.4874]])\n",
      "[-0.06065166]\n",
      "Mode: Train env_steps 200 total rewards -1762.9404892921448 total energy tensor([[15.2576]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:03<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2915301]\n",
      "Mode: Train env_steps 200 total rewards -1829.1677842140198 total energy tensor([[5.7955]])\n",
      "[-0.15265566]\n",
      "Mode: Train env_steps 200 total rewards -1932.0999507904053 total energy tensor([[1.7692]])\n",
      "[0.24700546]\n",
      "Mode: Train env_steps 200 total rewards -1934.891053199768 total energy tensor([[1.4986]])\n",
      "[-0.5331369]\n",
      "Mode: Train env_steps 200 total rewards -1360.5199131071568 total energy tensor([[42.8052]])\n",
      "[-0.4311398]\n",
      "Mode: Train env_steps 200 total rewards -1859.3657264709473 total energy tensor([[6.2059]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:32<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.65123814]\n",
      "Mode: Train env_steps 200 total rewards -1892.098575592041 total energy tensor([[2.8806]])\n",
      "[0.61828715]\n",
      "Mode: Train env_steps 200 total rewards -1846.9166860580444 total energy tensor([[3.8822]])\n",
      "[0.9394267]\n",
      "Mode: Train env_steps 200 total rewards -1927.112714767456 total energy tensor([[1.9169]])\n",
      "[-0.46418917]\n",
      "Mode: Train env_steps 200 total rewards -1730.5403471589088 total energy tensor([[18.7821]])\n",
      "[-0.6753547]\n",
      "Mode: Train env_steps 200 total rewards -1849.8407168388367 total energy tensor([[4.4013]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:31<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3209648]\n",
      "Mode: Test env_steps 200 total rewards -1369.8837891817093 total energy tensor([[40.4895]])\n",
      "[0.01294914]\n",
      "Mode: Test env_steps 200 total rewards -1616.8055165559053 total energy tensor([[22.6415]])\n",
      "[0.18893333]\n",
      "Mode: Test env_steps 200 total rewards -1936.345232963562 total energy tensor([[1.5555]])\n",
      "[0.66590136]\n",
      "Mode: Test env_steps 200 total rewards -1740.6114857196808 total energy tensor([[17.2951]])\n",
      "[0.07892353]\n",
      "Mode: Test env_steps 200 total rewards -1816.2199482917786 total energy tensor([[6.4498]])\n",
      "[0.3970958]\n",
      "Mode: Test env_steps 200 total rewards -1804.1469993591309 total energy tensor([[7.2913]])\n",
      "[-0.10368115]\n",
      "Mode: Test env_steps 200 total rewards -1768.8801229000092 total energy tensor([[15.9950]])\n",
      "[0.9485104]\n",
      "Mode: Test env_steps 200 total rewards -1804.7292461395264 total energy tensor([[7.2870]])\n",
      "[-0.4863958]\n",
      "Mode: Test env_steps 200 total rewards -1864.8785257339478 total energy tensor([[3.0738]])\n",
      "[-0.23219252]\n",
      "Mode: Test env_steps 200 total rewards -1367.0566866099834 total energy tensor([[40.1129]])\n",
      "40000 -1708.9557553455234\n",
      "[0.15241285]\n",
      "Mode: Train env_steps 200 total rewards -1897.320384502411 total energy tensor([[3.1516]])\n",
      "[-0.67811793]\n",
      "Mode: Train env_steps 200 total rewards -1698.799101114273 total energy tensor([[16.0942]])\n",
      "[-0.08924074]\n",
      "Mode: Train env_steps 200 total rewards -1885.8479490280151 total energy tensor([[3.5345]])\n",
      "[0.09414211]\n",
      "Mode: Train env_steps 200 total rewards -1917.1222524642944 total energy tensor([[2.5385]])\n",
      "[0.8006397]\n",
      "Mode: Train env_steps 200 total rewards -1839.1588797569275 total energy tensor([[8.0378]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:27<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.48512614]\n",
      "Mode: Train env_steps 200 total rewards -1869.8667387962341 total energy tensor([[4.2495]])\n",
      "[0.01395647]\n",
      "Mode: Train env_steps 200 total rewards -1852.211880683899 total energy tensor([[3.9831]])\n",
      "[0.8005497]\n",
      "Mode: Train env_steps 200 total rewards -1688.4643464684486 total energy tensor([[19.7121]])\n",
      "[0.04167321]\n",
      "Mode: Train env_steps 200 total rewards -1337.6573618352413 total energy tensor([[32.9300]])\n",
      "[0.88041]\n",
      "Mode: Train env_steps 200 total rewards -1924.427583694458 total energy tensor([[2.1114]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:28<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7780732]\n",
      "Mode: Train env_steps 200 total rewards -1344.4447799548507 total energy tensor([[36.4815]])\n",
      "[0.26227352]\n",
      "Mode: Train env_steps 200 total rewards -1360.3292890191078 total energy tensor([[36.8430]])\n",
      "[0.76572955]\n",
      "Mode: Train env_steps 200 total rewards -1846.6677474975586 total energy tensor([[3.7842]])\n",
      "[-0.42979556]\n",
      "Mode: Train env_steps 200 total rewards -1610.7319011762738 total energy tensor([[22.5358]])\n",
      "[0.92675865]\n",
      "Mode: Train env_steps 200 total rewards -1813.360577583313 total energy tensor([[6.2907]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:12<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62487334]\n",
      "Mode: Train env_steps 200 total rewards -1837.3510615825653 total energy tensor([[8.0110]])\n",
      "[0.01676218]\n",
      "Mode: Train env_steps 200 total rewards -1826.4575219154358 total energy tensor([[5.6816]])\n",
      "[-0.6192325]\n",
      "Mode: Train env_steps 200 total rewards -1691.1335198879242 total energy tensor([[20.2444]])\n",
      "[-0.46643832]\n",
      "Mode: Train env_steps 200 total rewards -1621.659099549055 total energy tensor([[22.0492]])\n",
      "[0.43142658]\n",
      "Mode: Train env_steps 200 total rewards -1688.535483598709 total energy tensor([[20.9313]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:05<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40483314]\n",
      "Mode: Train env_steps 200 total rewards -1830.3975019454956 total energy tensor([[5.9931]])\n",
      "[-0.20725653]\n",
      "Mode: Train env_steps 200 total rewards -1825.9507327079773 total energy tensor([[5.8567]])\n",
      "[-0.963758]\n",
      "Mode: Train env_steps 200 total rewards -1603.7994512729347 total energy tensor([[22.5407]])\n",
      "[0.97151953]\n",
      "Mode: Train env_steps 200 total rewards -1922.4610443115234 total energy tensor([[2.0780]])\n",
      "[0.6505613]\n",
      "Mode: Train env_steps 200 total rewards -1667.9489595293999 total energy tensor([[21.0821]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:28<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9294661]\n",
      "Mode: Test env_steps 200 total rewards -1794.4488258361816 total energy tensor([[12.8175]])\n",
      "[-0.7312869]\n",
      "Mode: Test env_steps 200 total rewards -1751.7645089626312 total energy tensor([[17.0941]])\n",
      "[-0.1943961]\n",
      "Mode: Test env_steps 200 total rewards -1892.730706691742 total energy tensor([[3.1584]])\n",
      "[-0.92642784]\n",
      "Mode: Test env_steps 200 total rewards -1685.2070016562939 total energy tensor([[21.5290]])\n",
      "[0.15604085]\n",
      "Mode: Test env_steps 200 total rewards -1212.982266236926 total energy tensor([[52.0378]])\n",
      "[0.04886438]\n",
      "Mode: Test env_steps 200 total rewards -1826.43745303154 total energy tensor([[9.4883]])\n",
      "[0.11433371]\n",
      "Mode: Test env_steps 200 total rewards -1730.2417036294937 total energy tensor([[18.6700]])\n",
      "[0.6184923]\n",
      "Mode: Test env_steps 200 total rewards -1639.8976250737906 total energy tensor([[22.8584]])\n",
      "[0.27098387]\n",
      "Mode: Test env_steps 200 total rewards -1358.5969622135162 total energy tensor([[37.3145]])\n",
      "[-0.90113866]\n",
      "Mode: Test env_steps 200 total rewards -1365.3653388619423 total energy tensor([[37.7789]])\n",
      "45000 -1625.7672392194058\n",
      "[-0.51241094]\n",
      "Mode: Train env_steps 200 total rewards -1449.6317748390138 total energy tensor([[24.1760]])\n",
      "[0.37514636]\n",
      "Mode: Train env_steps 200 total rewards -1630.0994524657726 total energy tensor([[22.7366]])\n",
      "[-0.89401245]\n",
      "Mode: Train env_steps 200 total rewards -1893.4724020957947 total energy tensor([[3.4161]])\n",
      "[-0.62190306]\n",
      "Mode: Train env_steps 200 total rewards -1629.940440312028 total energy tensor([[22.5598]])\n",
      "[0.83568937]\n",
      "Mode: Train env_steps 200 total rewards -1912.847746372223 total energy tensor([[2.8953]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:14<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03215145]\n",
      "Mode: Train env_steps 200 total rewards -1763.5642105340958 total energy tensor([[15.9738]])\n",
      "[0.37968466]\n",
      "Mode: Train env_steps 200 total rewards -1594.9742490947247 total energy tensor([[23.2374]])\n",
      "[-0.9605344]\n",
      "Mode: Train env_steps 200 total rewards -1669.8235552310944 total energy tensor([[17.7961]])\n",
      "[-0.98656315]\n",
      "Mode: Train env_steps 200 total rewards -1687.0672160685062 total energy tensor([[21.9719]])\n",
      "[0.9384674]\n",
      "Mode: Train env_steps 200 total rewards -1870.1859722137451 total energy tensor([[3.5319]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19892587]\n",
      "Mode: Train env_steps 200 total rewards -1921.2106113433838 total energy tensor([[2.7217]])\n",
      "[-0.49968842]\n",
      "Mode: Train env_steps 200 total rewards -1800.105731010437 total energy tensor([[8.1243]])\n",
      "[0.27044618]\n",
      "Mode: Train env_steps 200 total rewards -1752.942475438118 total energy tensor([[17.9294]])\n",
      "[0.44853464]\n",
      "Mode: Train env_steps 200 total rewards -1766.607182264328 total energy tensor([[15.5104]])\n",
      "[0.6325918]\n",
      "Mode: Train env_steps 200 total rewards -1346.0390987992287 total energy tensor([[33.7969]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15470915]\n",
      "Mode: Train env_steps 200 total rewards -1754.5353248119354 total energy tensor([[18.1794]])\n",
      "[-0.11353115]\n",
      "Mode: Train env_steps 200 total rewards -1636.1345685720444 total energy tensor([[19.7632]])\n",
      "[0.8591406]\n",
      "Mode: Train env_steps 200 total rewards -1781.519285917282 total energy tensor([[14.9460]])\n",
      "[0.46451038]\n",
      "Mode: Train env_steps 200 total rewards -1765.3818624019623 total energy tensor([[16.0401]])\n",
      "[-0.29733798]\n",
      "Mode: Train env_steps 200 total rewards -1673.1782869398594 total energy tensor([[22.4755]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:11<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40083376]\n",
      "Mode: Train env_steps 200 total rewards -1817.3572297096252 total energy tensor([[6.6563]])\n",
      "[-0.38210467]\n",
      "Mode: Train env_steps 200 total rewards -1808.561184644699 total energy tensor([[11.1071]])\n",
      "[-0.5189084]\n",
      "Mode: Train env_steps 200 total rewards -1742.7930958271027 total energy tensor([[17.7258]])\n",
      "[-0.5732252]\n",
      "Mode: Train env_steps 200 total rewards -1748.42533993721 total energy tensor([[11.9544]])\n",
      "[-0.81220526]\n",
      "Mode: Train env_steps 200 total rewards -1569.016594260931 total energy tensor([[22.0529]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:01<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42138013]\n",
      "Mode: Test env_steps 200 total rewards -1634.5783041119576 total energy tensor([[23.3151]])\n",
      "[0.24868236]\n",
      "Mode: Test env_steps 200 total rewards -1815.5684034824371 total energy tensor([[10.5327]])\n",
      "[0.32956746]\n",
      "Mode: Test env_steps 200 total rewards -1647.444678068161 total energy tensor([[18.2961]])\n",
      "[-0.20460892]\n",
      "Mode: Test env_steps 200 total rewards -1713.6535514593124 total energy tensor([[15.0601]])\n",
      "[-0.8836205]\n",
      "Mode: Test env_steps 200 total rewards -1475.1668321490288 total energy tensor([[23.0162]])\n",
      "[-0.03509248]\n",
      "Mode: Test env_steps 200 total rewards -1644.9461407065392 total energy tensor([[18.6980]])\n",
      "[-0.9097682]\n",
      "Mode: Test env_steps 200 total rewards -1649.12285810709 total energy tensor([[18.7078]])\n",
      "[-0.7100129]\n",
      "Mode: Test env_steps 200 total rewards -1800.6703231334686 total energy tensor([[11.3531]])\n",
      "[0.07539237]\n",
      "Mode: Test env_steps 200 total rewards -1896.7294402122498 total energy tensor([[3.0910]])\n",
      "[0.62226385]\n",
      "Mode: Test env_steps 200 total rewards -1715.7568354606628 total energy tensor([[14.6846]])\n",
      "50000 -1699.3637366890907\n",
      "[0.70578766]\n",
      "Mode: Train env_steps 200 total rewards -1540.560615569353 total energy tensor([[24.1233]])\n",
      "[0.6264265]\n",
      "Mode: Train env_steps 200 total rewards -1695.3039385080338 total energy tensor([[16.2385]])\n",
      "[-0.715268]\n",
      "Mode: Train env_steps 200 total rewards -1917.0487508773804 total energy tensor([[2.7205]])\n",
      "[0.5457226]\n",
      "Mode: Train env_steps 200 total rewards -1816.9690356254578 total energy tensor([[7.1647]])\n",
      "[0.7278698]\n",
      "Mode: Train env_steps 200 total rewards -1669.3401511907578 total energy tensor([[17.5311]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:57<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3133594]\n",
      "Mode: Train env_steps 200 total rewards -1698.3422666788101 total energy tensor([[21.6862]])\n",
      "[-0.064078]\n",
      "Mode: Train env_steps 200 total rewards -1651.8325768709183 total energy tensor([[23.1542]])\n",
      "[0.4338357]\n",
      "Mode: Train env_steps 200 total rewards -1647.60889005661 total energy tensor([[18.5043]])\n",
      "[-0.827236]\n",
      "Mode: Train env_steps 200 total rewards -1596.520895600319 total energy tensor([[20.1730]])\n",
      "[-0.4734519]\n",
      "Mode: Train env_steps 200 total rewards -1756.1398998498917 total energy tensor([[17.2874]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8420167]\n",
      "Mode: Train env_steps 200 total rewards -1728.2144443392754 total energy tensor([[20.6176]])\n",
      "[-0.06739818]\n",
      "Mode: Train env_steps 200 total rewards -1916.0685405731201 total energy tensor([[2.7834]])\n",
      "[0.9665999]\n",
      "Mode: Train env_steps 200 total rewards -1891.5223956108093 total energy tensor([[4.7018]])\n",
      "[0.266398]\n",
      "Mode: Train env_steps 200 total rewards -1814.6009550094604 total energy tensor([[6.9053]])\n",
      "[-0.26638687]\n",
      "Mode: Train env_steps 200 total rewards -1748.4988219738007 total energy tensor([[11.4028]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:20<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7994443]\n",
      "Mode: Train env_steps 200 total rewards -1907.8679866790771 total energy tensor([[3.9357]])\n",
      "[0.3618125]\n",
      "Mode: Train env_steps 200 total rewards -1638.8415734916925 total energy tensor([[24.7174]])\n",
      "[-0.08971651]\n",
      "Mode: Train env_steps 200 total rewards -1697.8917961716652 total energy tensor([[21.9379]])\n",
      "[0.42836937]\n",
      "Mode: Train env_steps 200 total rewards -1691.423469632864 total energy tensor([[23.6773]])\n",
      "[0.5888775]\n",
      "Mode: Train env_steps 200 total rewards -1824.5532875061035 total energy tensor([[6.4701]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35192233]\n",
      "Mode: Train env_steps 200 total rewards -1788.156792640686 total energy tensor([[8.8831]])\n",
      "[0.65618426]\n",
      "Mode: Train env_steps 200 total rewards -1767.7417907714844 total energy tensor([[10.9214]])\n",
      "[0.2870116]\n",
      "Mode: Train env_steps 200 total rewards -1759.3531818389893 total energy tensor([[18.2868]])\n",
      "[0.81198263]\n",
      "Mode: Train env_steps 200 total rewards -1682.2223567962646 total energy tensor([[17.1349]])\n",
      "[-0.3136218]\n",
      "Mode: Train env_steps 200 total rewards -1648.3606014847755 total energy tensor([[24.3034]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:27<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01630884]\n",
      "Mode: Test env_steps 200 total rewards -1728.0063247680664 total energy tensor([[14.7074]])\n",
      "[0.714346]\n",
      "Mode: Test env_steps 200 total rewards -1869.8560552597046 total energy tensor([[5.8410]])\n",
      "[-0.61074173]\n",
      "Mode: Test env_steps 200 total rewards -1590.863879173994 total energy tensor([[25.1340]])\n",
      "[0.11786392]\n",
      "Mode: Test env_steps 200 total rewards -1894.1665935516357 total energy tensor([[4.6353]])\n",
      "[-0.9872365]\n",
      "Mode: Test env_steps 200 total rewards -1772.0067151784897 total energy tensor([[16.4093]])\n",
      "[-0.83762914]\n",
      "Mode: Test env_steps 200 total rewards -1674.604913175106 total energy tensor([[22.9792]])\n",
      "[0.76089066]\n",
      "Mode: Test env_steps 200 total rewards -1689.0073791444302 total energy tensor([[23.0642]])\n",
      "[0.4961847]\n",
      "Mode: Test env_steps 200 total rewards -1078.8600202668458 total energy tensor([[25.7426]])\n",
      "[-0.78153837]\n",
      "Mode: Test env_steps 200 total rewards -1068.250623865053 total energy tensor([[15.0240]])\n",
      "[-0.24437606]\n",
      "Mode: Test env_steps 200 total rewards -1721.1471420526505 total energy tensor([[20.1137]])\n",
      "55000 -1608.6769646435976\n",
      "[0.36916474]\n",
      "Mode: Train env_steps 200 total rewards -1711.1595034599304 total energy tensor([[15.7644]])\n",
      "[-0.4047819]\n",
      "Mode: Train env_steps 200 total rewards -1648.8041523098946 total energy tensor([[24.5765]])\n",
      "[0.47380522]\n",
      "Mode: Train env_steps 200 total rewards -1436.7893101274967 total energy tensor([[24.1310]])\n",
      "[-0.49196693]\n",
      "Mode: Train env_steps 200 total rewards -1839.6284198760986 total energy tensor([[5.6479]])\n",
      "[-0.4189723]\n",
      "Mode: Train env_steps 200 total rewards -1813.7523646354675 total energy tensor([[12.2043]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:24<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9658687]\n",
      "Mode: Train env_steps 200 total rewards -1770.288536310196 total energy tensor([[10.4851]])\n",
      "[0.10595401]\n",
      "Mode: Train env_steps 200 total rewards -1899.2884311676025 total energy tensor([[4.4941]])\n",
      "[-0.8950067]\n",
      "Mode: Train env_steps 200 total rewards -1789.4617371559143 total energy tensor([[8.5539]])\n",
      "[0.811244]\n",
      "Mode: Train env_steps 200 total rewards -1852.5565347671509 total energy tensor([[7.1328]])\n",
      "[0.97951186]\n",
      "Mode: Train env_steps 200 total rewards -1751.2616112232208 total energy tensor([[18.8907]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:08<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23450509]\n",
      "Mode: Train env_steps 200 total rewards -1785.4768905639648 total energy tensor([[16.3654]])\n",
      "[-0.06969831]\n",
      "Mode: Train env_steps 200 total rewards -1787.161955833435 total energy tensor([[9.1735]])\n",
      "[-0.4703267]\n",
      "Mode: Train env_steps 200 total rewards -1752.6586272716522 total energy tensor([[13.1645]])\n",
      "[0.675857]\n",
      "Mode: Train env_steps 200 total rewards -1768.082941532135 total energy tensor([[17.6717]])\n",
      "[0.57088923]\n",
      "Mode: Train env_steps 200 total rewards -1833.9319953918457 total energy tensor([[6.1531]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:35<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9406593]\n",
      "Mode: Train env_steps 200 total rewards -1705.8267886638641 total energy tensor([[20.9618]])\n",
      "[0.16843854]\n",
      "Mode: Train env_steps 200 total rewards -1763.684918642044 total energy tensor([[11.4964]])\n",
      "[-0.6534672]\n",
      "Mode: Train env_steps 200 total rewards -1699.9360486268997 total energy tensor([[21.0698]])\n",
      "[-0.16248064]\n",
      "Mode: Train env_steps 200 total rewards -1780.2045214176178 total energy tensor([[15.5238]])\n",
      "[0.5200351]\n",
      "Mode: Train env_steps 200 total rewards -1700.7555345892906 total energy tensor([[22.3137]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48182556]\n",
      "Mode: Train env_steps 200 total rewards -1829.6474394798279 total energy tensor([[9.2077]])\n",
      "[-0.7505483]\n",
      "Mode: Train env_steps 200 total rewards -1501.322518227622 total energy tensor([[25.9562]])\n",
      "[-0.5746645]\n",
      "Mode: Train env_steps 200 total rewards -1898.6433601379395 total energy tensor([[4.6230]])\n",
      "[0.9079027]\n",
      "Mode: Train env_steps 200 total rewards -1893.7273273468018 total energy tensor([[4.7993]])\n",
      "[0.04577384]\n",
      "Mode: Train env_steps 200 total rewards -1882.9579162597656 total energy tensor([[5.7643]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:27<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3684391]\n",
      "Mode: Test env_steps 200 total rewards -1718.3622078895569 total energy tensor([[15.5243]])\n",
      "[-0.50549036]\n",
      "Mode: Test env_steps 200 total rewards -1756.4933975934982 total energy tensor([[17.8935]])\n",
      "[-0.8859417]\n",
      "Mode: Test env_steps 200 total rewards -1081.8314996326808 total energy tensor([[43.5625]])\n",
      "[-0.4392322]\n",
      "Mode: Test env_steps 200 total rewards -1796.6482424736023 total energy tensor([[14.6594]])\n",
      "[0.79719615]\n",
      "Mode: Test env_steps 200 total rewards -1814.8962988853455 total energy tensor([[10.6768]])\n",
      "[-0.3452081]\n",
      "Mode: Test env_steps 200 total rewards -1431.2141695693135 total energy tensor([[25.3150]])\n",
      "[-0.37021962]\n",
      "Mode: Test env_steps 200 total rewards -1585.9295133054256 total energy tensor([[25.7228]])\n",
      "[0.25651783]\n",
      "Mode: Test env_steps 200 total rewards -1885.1793332099915 total energy tensor([[5.8435]])\n",
      "[-0.5278507]\n",
      "Mode: Test env_steps 200 total rewards -1705.4498327374458 total energy tensor([[21.5511]])\n",
      "[0.48406732]\n",
      "Mode: Test env_steps 200 total rewards -1100.9487102944404 total energy tensor([[27.3479]])\n",
      "60000 -1587.69532055913\n",
      "[-0.68106884]\n",
      "Mode: Train env_steps 200 total rewards -1735.4775737524033 total energy tensor([[20.6729]])\n",
      "[0.12009176]\n",
      "Mode: Train env_steps 200 total rewards -1879.8962602615356 total energy tensor([[6.0019]])\n",
      "[-0.29629067]\n",
      "Mode: Train env_steps 200 total rewards -1794.2133221626282 total energy tensor([[15.1166]])\n",
      "[0.6211933]\n",
      "Mode: Train env_steps 200 total rewards -1657.1504254043102 total energy tensor([[23.9304]])\n",
      "[0.28768578]\n",
      "Mode: Train env_steps 200 total rewards -1648.9972981512547 total energy tensor([[25.9427]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:23<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01777115]\n",
      "Mode: Train env_steps 200 total rewards -1882.808439731598 total energy tensor([[5.5857]])\n",
      "[0.5934154]\n",
      "Mode: Train env_steps 200 total rewards -1547.103693574667 total energy tensor([[22.2052]])\n",
      "[-0.18652074]\n",
      "Mode: Train env_steps 200 total rewards -1619.8164192289114 total energy tensor([[24.4869]])\n",
      "[-0.2460178]\n",
      "Mode: Train env_steps 200 total rewards -1478.2938439399004 total energy tensor([[23.3450]])\n",
      "[-0.5459168]\n",
      "Mode: Train env_steps 200 total rewards -1503.178875476122 total energy tensor([[23.3985]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07483624]\n",
      "Mode: Train env_steps 200 total rewards -1894.4841966629028 total energy tensor([[5.2078]])\n",
      "[-0.9107214]\n",
      "Mode: Train env_steps 200 total rewards -1689.3099572658539 total energy tensor([[17.4299]])\n",
      "[0.77721685]\n",
      "Mode: Train env_steps 200 total rewards -1739.3083727359772 total energy tensor([[14.3348]])\n",
      "[0.96681947]\n",
      "Mode: Train env_steps 200 total rewards -1841.6399760246277 total energy tensor([[6.6058]])\n",
      "[0.15856276]\n",
      "Mode: Train env_steps 200 total rewards -1862.6544589996338 total energy tensor([[6.6036]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:31<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.924144]\n",
      "Mode: Train env_steps 200 total rewards -1709.4603998661041 total energy tensor([[16.9927]])\n",
      "[0.07047912]\n",
      "Mode: Train env_steps 200 total rewards -1888.7674379348755 total energy tensor([[5.8569]])\n",
      "[0.43224794]\n",
      "Mode: Train env_steps 200 total rewards -1868.7845206260681 total energy tensor([[6.8133]])\n",
      "[-0.8931884]\n",
      "Mode: Train env_steps 200 total rewards -1609.1117327213287 total energy tensor([[21.2681]])\n",
      "[0.22342257]\n",
      "Mode: Train env_steps 200 total rewards -1888.7458410263062 total energy tensor([[5.5843]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:17<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9129607]\n",
      "Mode: Train env_steps 200 total rewards -1759.1311700344086 total energy tensor([[12.2058]])\n",
      "[0.24659434]\n",
      "Mode: Train env_steps 200 total rewards -1680.371215224266 total energy tensor([[18.4299]])\n",
      "[-0.78240466]\n",
      "Mode: Train env_steps 200 total rewards -1876.6994976997375 total energy tensor([[6.5436]])\n",
      "[-0.55695796]\n",
      "Mode: Train env_steps 200 total rewards -1535.3742465376854 total energy tensor([[22.0710]])\n",
      "[-0.43081352]\n",
      "Mode: Train env_steps 200 total rewards -1890.0721940994263 total energy tensor([[5.3844]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:29<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8268217]\n",
      "Mode: Test env_steps 200 total rewards -1607.0474002957344 total energy tensor([[21.0182]])\n",
      "[-0.4456242]\n",
      "Mode: Test env_steps 200 total rewards -1719.5570878982544 total energy tensor([[21.6744]])\n",
      "[0.82562166]\n",
      "Mode: Test env_steps 200 total rewards -1824.2567782402039 total energy tensor([[9.6397]])\n",
      "[-0.27271053]\n",
      "Mode: Test env_steps 200 total rewards -1863.4001088142395 total energy tensor([[7.0729]])\n",
      "[0.14387952]\n",
      "Mode: Test env_steps 200 total rewards -1837.6430864334106 total energy tensor([[6.9459]])\n",
      "[-0.0277853]\n",
      "Mode: Test env_steps 200 total rewards -1751.2086639404297 total energy tensor([[13.0260]])\n",
      "[-0.6834483]\n",
      "Mode: Test env_steps 200 total rewards -1877.7356896400452 total energy tensor([[6.2967]])\n",
      "[0.77873504]\n",
      "Mode: Test env_steps 200 total rewards -1867.2845859527588 total energy tensor([[5.6448]])\n",
      "[0.9550433]\n",
      "Mode: Test env_steps 200 total rewards -1455.6705842381343 total energy tensor([[21.9147]])\n",
      "[-0.46722084]\n",
      "Mode: Test env_steps 200 total rewards -1806.8213016986847 total energy tensor([[11.8682]])\n",
      "65000 -1761.0625287151895\n",
      "[-0.21849155]\n",
      "Mode: Train env_steps 200 total rewards -1536.0582601465285 total energy tensor([[22.2372]])\n",
      "[-0.63872266]\n",
      "Mode: Train env_steps 200 total rewards -1744.4839062690735 total energy tensor([[13.9757]])\n",
      "[-0.7252642]\n",
      "Mode: Train env_steps 200 total rewards -1823.542329788208 total energy tensor([[7.5637]])\n",
      "[0.8143108]\n",
      "Mode: Train env_steps 200 total rewards -1767.2495303153992 total energy tensor([[11.5623]])\n",
      "[-0.32313502]\n",
      "Mode: Train env_steps 200 total rewards -1857.3348469734192 total energy tensor([[7.8895]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:11<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37320453]\n",
      "Mode: Train env_steps 200 total rewards -1857.6903281211853 total energy tensor([[7.7772]])\n",
      "[-0.7474673]\n",
      "Mode: Train env_steps 200 total rewards -1883.5286779403687 total energy tensor([[5.2306]])\n",
      "[0.64353734]\n",
      "Mode: Train env_steps 200 total rewards -1891.1976299285889 total energy tensor([[5.1807]])\n",
      "[-0.954044]\n",
      "Mode: Train env_steps 200 total rewards -1888.1331100463867 total energy tensor([[5.1715]])\n",
      "[0.78230184]\n",
      "Mode: Train env_steps 200 total rewards -1510.7169608920813 total energy tensor([[22.2622]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:06<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9472982]\n",
      "Mode: Train env_steps 200 total rewards -1847.8469505310059 total energy tensor([[7.8992]])\n",
      "[0.68912]\n",
      "Mode: Train env_steps 200 total rewards -1715.0657708644867 total energy tensor([[16.0058]])\n",
      "[0.50833637]\n",
      "Mode: Train env_steps 200 total rewards -1631.4957299232483 total energy tensor([[20.5205]])\n",
      "[-0.08532714]\n",
      "Mode: Train env_steps 200 total rewards -1874.9423441886902 total energy tensor([[6.0344]])\n",
      "[-0.9362373]\n",
      "Mode: Train env_steps 200 total rewards -1842.7410173416138 total energy tensor([[6.4292]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:15<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29513547]\n",
      "Mode: Train env_steps 200 total rewards -1633.791264295578 total energy tensor([[20.0520]])\n",
      "[-0.18207656]\n",
      "Mode: Train env_steps 200 total rewards -1859.0448513031006 total energy tensor([[7.5212]])\n",
      "[0.75788873]\n",
      "Mode: Train env_steps 200 total rewards -1854.695430278778 total energy tensor([[7.8118]])\n",
      "[-0.37232405]\n",
      "Mode: Train env_steps 200 total rewards -1679.564288109541 total energy tensor([[23.4345]])\n",
      "[0.05506822]\n",
      "Mode: Train env_steps 200 total rewards -1413.496458229376 total energy tensor([[21.6814]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:58<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2108131]\n",
      "Mode: Train env_steps 200 total rewards -1874.3747806549072 total energy tensor([[5.1773]])\n",
      "[0.49056044]\n",
      "Mode: Train env_steps 200 total rewards -1840.2085852622986 total energy tensor([[6.4143]])\n",
      "[0.85392696]\n",
      "Mode: Train env_steps 200 total rewards -1626.4442472159863 total energy tensor([[20.0872]])\n",
      "[-0.4948401]\n",
      "Mode: Train env_steps 200 total rewards -1865.585114955902 total energy tensor([[6.6041]])\n",
      "[0.7162319]\n",
      "Mode: Train env_steps 200 total rewards -1455.1228447244503 total energy tensor([[20.6505]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:55<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06211038]\n",
      "Mode: Test env_steps 200 total rewards -1674.2429419755936 total energy tensor([[22.0888]])\n",
      "[-0.5015586]\n",
      "Mode: Test env_steps 200 total rewards -1888.6661710739136 total energy tensor([[5.7555]])\n",
      "[-0.40954563]\n",
      "Mode: Test env_steps 200 total rewards -1777.976316690445 total energy tensor([[16.6225]])\n",
      "[-0.8162156]\n",
      "Mode: Test env_steps 200 total rewards -1885.882703781128 total energy tensor([[5.9479]])\n",
      "[0.57959235]\n",
      "Mode: Test env_steps 200 total rewards -1640.794600263238 total energy tensor([[23.5107]])\n",
      "[0.12218763]\n",
      "Mode: Test env_steps 200 total rewards -1868.1904835700989 total energy tensor([[6.4045]])\n",
      "[0.43719727]\n",
      "Mode: Test env_steps 200 total rewards -1625.4639767855406 total energy tensor([[23.8896]])\n",
      "[0.9824797]\n",
      "Mode: Test env_steps 200 total rewards -1877.8864660263062 total energy tensor([[5.9212]])\n",
      "[0.44532087]\n",
      "Mode: Test env_steps 200 total rewards -1743.9127215147018 total energy tensor([[18.7627]])\n",
      "[0.04631947]\n",
      "Mode: Test env_steps 200 total rewards -1585.0781296938658 total energy tensor([[20.9769]])\n",
      "70000 -1756.8094511374832\n",
      "[0.5790493]\n",
      "Mode: Train env_steps 200 total rewards -1869.2209062576294 total energy tensor([[6.1980]])\n",
      "[0.350429]\n",
      "Mode: Train env_steps 200 total rewards -1477.3186851274222 total energy tensor([[24.5828]])\n",
      "[0.1800314]\n",
      "Mode: Train env_steps 200 total rewards -1874.1007137298584 total energy tensor([[6.1107]])\n",
      "[-0.41647634]\n",
      "Mode: Train env_steps 200 total rewards -1854.1126670837402 total energy tensor([[6.3468]])\n",
      "[0.48268372]\n",
      "Mode: Train env_steps 200 total rewards -1718.0275421142578 total energy tensor([[15.2959]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:59<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7235859]\n",
      "Mode: Train env_steps 200 total rewards -1817.38223361969 total energy tensor([[10.6294]])\n",
      "[-0.65142477]\n",
      "Mode: Train env_steps 200 total rewards -1870.8636837005615 total energy tensor([[6.8020]])\n",
      "[0.12123479]\n",
      "Mode: Train env_steps 200 total rewards -1839.8687510490417 total energy tensor([[6.5130]])\n",
      "[-0.21346535]\n",
      "Mode: Train env_steps 200 total rewards -1707.564540863037 total energy tensor([[21.4003]])\n",
      "[-0.43562165]\n",
      "Mode: Train env_steps 200 total rewards -1698.3652935028076 total energy tensor([[20.9759]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:59<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18918186]\n",
      "Mode: Train env_steps 200 total rewards -1685.7435510754585 total energy tensor([[21.7995]])\n",
      "[0.854785]\n",
      "Mode: Train env_steps 200 total rewards -1813.446389913559 total energy tensor([[10.9203]])\n",
      "[-0.64358103]\n",
      "Mode: Train env_steps 200 total rewards -1618.898284368217 total energy tensor([[23.7165]])\n",
      "[0.14829029]\n",
      "Mode: Train env_steps 200 total rewards -1881.4479675292969 total energy tensor([[5.0057]])\n",
      "[-0.57928896]\n",
      "Mode: Train env_steps 200 total rewards -1869.0356726646423 total energy tensor([[6.7660]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:56<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12600864]\n",
      "Mode: Train env_steps 200 total rewards -1585.7684149630368 total energy tensor([[23.2440]])\n",
      "[-0.05172486]\n",
      "Mode: Train env_steps 200 total rewards -1665.9182832092047 total energy tensor([[22.5437]])\n",
      "[0.4578488]\n",
      "Mode: Train env_steps 200 total rewards -1726.3103192448616 total energy tensor([[19.6049]])\n",
      "[0.02103662]\n",
      "Mode: Train env_steps 200 total rewards -1810.1864655017853 total energy tensor([[10.3641]])\n",
      "[-0.9548587]\n",
      "Mode: Train env_steps 200 total rewards -1871.5265169143677 total energy tensor([[6.4505]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:57<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00555227]\n",
      "Mode: Train env_steps 200 total rewards -1625.982881128788 total energy tensor([[19.3411]])\n",
      "[0.2166729]\n",
      "Mode: Train env_steps 200 total rewards -1895.0644779205322 total energy tensor([[5.2001]])\n",
      "[-0.89542997]\n",
      "Mode: Train env_steps 200 total rewards -1273.180535239284 total energy tensor([[19.5657]])\n",
      "[0.7166959]\n",
      "Mode: Train env_steps 200 total rewards -1654.420184865594 total energy tensor([[23.2655]])\n",
      "[-0.68611664]\n",
      "Mode: Train env_steps 200 total rewards -1827.015215396881 total energy tensor([[7.0278]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:06<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07304686]\n",
      "Mode: Test env_steps 200 total rewards -1709.90606752038 total energy tensor([[21.5468]])\n",
      "[0.36865124]\n",
      "Mode: Test env_steps 200 total rewards -1628.1127270162106 total energy tensor([[18.9809]])\n",
      "[0.9273282]\n",
      "Mode: Test env_steps 200 total rewards -1735.8824939727783 total energy tensor([[12.9845]])\n",
      "[-0.9926403]\n",
      "Mode: Test env_steps 200 total rewards -1599.5709739923477 total energy tensor([[19.5713]])\n",
      "[0.3269941]\n",
      "Mode: Test env_steps 200 total rewards -1831.3901863098145 total energy tensor([[9.2692]])\n",
      "[0.3248072]\n",
      "Mode: Test env_steps 200 total rewards -1872.246822834015 total energy tensor([[6.5007]])\n",
      "[0.9669345]\n",
      "Mode: Test env_steps 200 total rewards -1702.610684543848 total energy tensor([[21.4261]])\n",
      "[-0.51329315]\n",
      "Mode: Test env_steps 200 total rewards -1596.7991974186152 total energy tensor([[23.5029]])\n",
      "[-0.24647936]\n",
      "Mode: Test env_steps 200 total rewards -1737.7948627471924 total energy tensor([[13.4197]])\n",
      "[-0.27235004]\n",
      "Mode: Test env_steps 200 total rewards -1707.5032116174698 total energy tensor([[15.7568]])\n",
      "75000 -1712.1817227972672\n",
      "[-0.3260055]\n",
      "Mode: Train env_steps 200 total rewards -1589.9562837593257 total energy tensor([[23.8276]])\n",
      "[-0.45984742]\n",
      "Mode: Train env_steps 200 total rewards -1748.8441644906998 total energy tensor([[19.2651]])\n",
      "[-0.80575114]\n",
      "Mode: Train env_steps 200 total rewards -1804.120921611786 total energy tensor([[12.1045]])\n",
      "[0.8257522]\n",
      "Mode: Train env_steps 200 total rewards -1692.194412380457 total energy tensor([[22.1711]])\n",
      "[-0.6390153]\n",
      "Mode: Train env_steps 200 total rewards -1820.7115831375122 total energy tensor([[10.0220]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:00<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.84402037]\n",
      "Mode: Train env_steps 200 total rewards -1806.0136642456055 total energy tensor([[8.2605]])\n",
      "[-0.6368398]\n",
      "Mode: Train env_steps 200 total rewards -1803.9449417591095 total energy tensor([[12.9254]])\n",
      "[0.00493748]\n",
      "Mode: Train env_steps 200 total rewards -1693.1911097168922 total energy tensor([[21.5754]])\n",
      "[0.8123643]\n",
      "Mode: Train env_steps 200 total rewards -1841.6359395980835 total energy tensor([[6.9550]])\n",
      "[-0.77822036]\n",
      "Mode: Train env_steps 200 total rewards -1743.860538482666 total energy tensor([[12.8635]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:02<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78727096]\n",
      "Mode: Train env_steps 200 total rewards -1658.8317453116179 total energy tensor([[22.9203]])\n",
      "[0.7445667]\n",
      "Mode: Train env_steps 200 total rewards -1818.6616978645325 total energy tensor([[11.0668]])\n",
      "[0.876291]\n",
      "Mode: Train env_steps 200 total rewards -1866.743191242218 total energy tensor([[6.5725]])\n",
      "[0.07413258]\n",
      "Mode: Train env_steps 200 total rewards -1733.3551495075226 total energy tensor([[19.4209]])\n",
      "[0.62304515]\n",
      "Mode: Train env_steps 200 total rewards -1860.3973054885864 total energy tensor([[7.0482]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:00<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42767766]\n",
      "Mode: Train env_steps 200 total rewards -1693.8138190507889 total energy tensor([[21.1214]])\n",
      "[-0.01018314]\n",
      "Mode: Train env_steps 200 total rewards -1687.3270101547241 total energy tensor([[16.6058]])\n",
      "[-0.10516604]\n",
      "Mode: Train env_steps 200 total rewards -1894.3571681976318 total energy tensor([[4.8341]])\n",
      "[-0.07013538]\n",
      "Mode: Train env_steps 200 total rewards -1887.9142799377441 total energy tensor([[5.7829]])\n",
      "[-0.3391724]\n",
      "Mode: Train env_steps 200 total rewards -1651.2111244797707 total energy tensor([[18.5272]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:04<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.34505045]\n",
      "Mode: Train env_steps 200 total rewards -1864.3985342979431 total energy tensor([[5.8294]])\n",
      "[-0.9426343]\n",
      "Mode: Train env_steps 200 total rewards -1695.6102533340454 total energy tensor([[15.9131]])\n",
      "[0.4471411]\n",
      "Mode: Train env_steps 200 total rewards -1741.1736228466034 total energy tensor([[12.9883]])\n",
      "[-0.68376267]\n",
      "Mode: Train env_steps 200 total rewards -1713.969077706337 total energy tensor([[14.4458]])\n",
      "[-0.3792988]\n",
      "Mode: Train env_steps 200 total rewards -1895.9311037063599 total energy tensor([[5.0580]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:04<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35615048]\n",
      "Mode: Test env_steps 200 total rewards -1838.5456671714783 total energy tensor([[8.5278]])\n",
      "[-0.06528242]\n",
      "Mode: Test env_steps 200 total rewards -1808.708116054535 total energy tensor([[12.6041]])\n",
      "[-0.23765643]\n",
      "Mode: Test env_steps 200 total rewards -1765.428864479065 total energy tensor([[16.1431]])\n",
      "[-0.7390572]\n",
      "Mode: Test env_steps 200 total rewards -1843.9335017204285 total energy tensor([[6.5740]])\n",
      "[0.48510474]\n",
      "Mode: Test env_steps 200 total rewards -1776.9436584711075 total energy tensor([[16.8383]])\n",
      "[0.68000424]\n",
      "Mode: Test env_steps 200 total rewards -1859.2275185585022 total energy tensor([[6.8379]])\n",
      "[-0.63240695]\n",
      "Mode: Test env_steps 200 total rewards -1705.0951740145683 total energy tensor([[21.7580]])\n",
      "[-0.888294]\n",
      "Mode: Test env_steps 200 total rewards -1703.445943057537 total energy tensor([[21.0036]])\n",
      "[-0.8416811]\n",
      "Mode: Test env_steps 200 total rewards -1885.7819991111755 total energy tensor([[5.3644]])\n",
      "[-0.60519856]\n",
      "Mode: Test env_steps 200 total rewards -1626.45526227355 total energy tensor([[19.0728]])\n",
      "80000 -1781.3565704911948\n",
      "[0.43750826]\n",
      "Mode: Train env_steps 200 total rewards -1719.186699271202 total energy tensor([[15.0964]])\n",
      "[-0.44578475]\n",
      "Mode: Train env_steps 200 total rewards -1696.7807618379593 total energy tensor([[16.6289]])\n",
      "[-0.08935444]\n",
      "Mode: Train env_steps 200 total rewards -1795.42635679245 total energy tensor([[12.9766]])\n",
      "[-0.57435113]\n",
      "Mode: Train env_steps 200 total rewards -1773.3641681671143 total energy tensor([[10.8372]])\n",
      "[0.09884002]\n",
      "Mode: Train env_steps 200 total rewards -1861.0695400238037 total energy tensor([[5.8081]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:00<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6594014]\n",
      "Mode: Train env_steps 200 total rewards -1773.7827732563019 total energy tensor([[17.2786]])\n",
      "[-0.5600553]\n",
      "Mode: Train env_steps 200 total rewards -1887.0171546936035 total energy tensor([[5.2940]])\n",
      "[-0.13972579]\n",
      "Mode: Train env_steps 200 total rewards -1718.3252382278442 total energy tensor([[20.5522]])\n",
      "[-0.8907427]\n",
      "Mode: Train env_steps 200 total rewards -1827.21910572052 total energy tensor([[9.6455]])\n",
      "[0.42096418]\n",
      "Mode: Train env_steps 200 total rewards -1801.3982481956482 total energy tensor([[13.8632]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:01<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78358465]\n",
      "Mode: Train env_steps 200 total rewards -1748.717424273491 total energy tensor([[18.0453]])\n",
      "[0.47809154]\n",
      "Mode: Train env_steps 200 total rewards -1735.0994296073914 total energy tensor([[14.9886]])\n",
      "[0.9257999]\n",
      "Mode: Train env_steps 200 total rewards -1800.11505818367 total energy tensor([[9.4852]])\n",
      "[-0.4304514]\n",
      "Mode: Train env_steps 200 total rewards -1672.4359190762043 total energy tensor([[17.7829]])\n",
      "[0.35999745]\n",
      "Mode: Train env_steps 200 total rewards -1840.703784942627 total energy tensor([[8.1511]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3135557]\n",
      "Mode: Train env_steps 200 total rewards -1854.0456309318542 total energy tensor([[7.1985]])\n",
      "[0.14819355]\n",
      "Mode: Train env_steps 200 total rewards -1767.7223320007324 total energy tensor([[16.8119]])\n",
      "[0.13536538]\n",
      "Mode: Train env_steps 200 total rewards -1697.4164431095123 total energy tensor([[17.5513]])\n",
      "[-0.0339441]\n",
      "Mode: Train env_steps 200 total rewards -1843.021665096283 total energy tensor([[8.5615]])\n",
      "[-0.852765]\n",
      "Mode: Train env_steps 200 total rewards -1543.1082634981722 total energy tensor([[19.3923]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:20<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8173576]\n",
      "Mode: Train env_steps 200 total rewards -1848.9218034744263 total energy tensor([[7.4231]])\n",
      "[0.94750744]\n",
      "Mode: Train env_steps 200 total rewards -1830.8810935020447 total energy tensor([[9.4044]])\n",
      "[-0.88215077]\n",
      "Mode: Train env_steps 200 total rewards -1594.619949825108 total energy tensor([[22.4550]])\n",
      "[-0.50952935]\n",
      "Mode: Train env_steps 200 total rewards -1718.668552994728 total energy tensor([[16.1897]])\n",
      "[-0.6355062]\n",
      "Mode: Train env_steps 200 total rewards -1676.6784285604954 total energy tensor([[18.0042]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:21<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23446889]\n",
      "Mode: Test env_steps 200 total rewards -1706.7563310861588 total energy tensor([[22.2929]])\n",
      "[-0.24592978]\n",
      "Mode: Test env_steps 200 total rewards -1790.9555678367615 total energy tensor([[10.4631]])\n",
      "[0.71214813]\n",
      "Mode: Test env_steps 200 total rewards -1770.6086971759796 total energy tensor([[12.7825]])\n",
      "[0.66145694]\n",
      "Mode: Test env_steps 200 total rewards -1755.1671330928802 total energy tensor([[18.1660]])\n",
      "[-0.5473258]\n",
      "Mode: Test env_steps 200 total rewards -1773.2178152799606 total energy tensor([[16.6080]])\n",
      "[-0.8434899]\n",
      "Mode: Test env_steps 200 total rewards -1681.2480400949717 total energy tensor([[21.8707]])\n",
      "[-0.1472114]\n",
      "Mode: Test env_steps 200 total rewards -1698.3388919234276 total energy tensor([[22.3641]])\n",
      "[-0.7055109]\n",
      "Mode: Test env_steps 200 total rewards -1715.5717427432537 total energy tensor([[21.0140]])\n",
      "[0.08159346]\n",
      "Mode: Test env_steps 200 total rewards -1766.989174604416 total energy tensor([[13.4119]])\n",
      "[-0.76357424]\n",
      "Mode: Test env_steps 200 total rewards -1563.5278949439526 total energy tensor([[19.6445]])\n",
      "85000 -1722.2381288781762\n",
      "[0.7791656]\n",
      "Mode: Train env_steps 200 total rewards -1733.8302727937698 total energy tensor([[16.5703]])\n",
      "[-0.69894606]\n",
      "Mode: Train env_steps 200 total rewards -1600.1755115389824 total energy tensor([[19.7278]])\n",
      "[0.7127756]\n",
      "Mode: Train env_steps 200 total rewards -1708.2751191854477 total energy tensor([[17.3108]])\n",
      "[0.18496981]\n",
      "Mode: Train env_steps 200 total rewards -1894.8937759399414 total energy tensor([[4.6443]])\n",
      "[0.38206157]\n",
      "Mode: Train env_steps 200 total rewards -1677.7552069425583 total energy tensor([[18.8582]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:19<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9628665]\n",
      "Mode: Train env_steps 200 total rewards -1853.375147819519 total energy tensor([[8.0447]])\n",
      "[-0.3274088]\n",
      "Mode: Train env_steps 200 total rewards -1677.6461918354034 total energy tensor([[18.3655]])\n",
      "[0.20950559]\n",
      "Mode: Train env_steps 200 total rewards -1793.415320634842 total energy tensor([[11.2110]])\n",
      "[-0.42384756]\n",
      "Mode: Train env_steps 200 total rewards -1504.3804801884107 total energy tensor([[19.5158]])\n",
      "[0.08419411]\n",
      "Mode: Train env_steps 200 total rewards -1707.6612555086613 total energy tensor([[21.1630]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9867461]\n",
      "Mode: Train env_steps 200 total rewards -1443.0670258390019 total energy tensor([[19.6722]])\n",
      "[0.7147748]\n",
      "Mode: Train env_steps 200 total rewards -1598.1803290769458 total energy tensor([[19.7722]])\n",
      "[-0.7663517]\n",
      "Mode: Train env_steps 200 total rewards -1828.167981147766 total energy tensor([[7.6853]])\n",
      "[0.45027757]\n",
      "Mode: Train env_steps 200 total rewards -1897.9874668121338 total energy tensor([[4.9732]])\n",
      "[-0.78078043]\n",
      "Mode: Train env_steps 200 total rewards -1721.6121042966843 total energy tensor([[16.8362]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:06<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36992982]\n",
      "Mode: Train env_steps 200 total rewards -1646.7269360795617 total energy tensor([[23.2848]])\n",
      "[0.60867023]\n",
      "Mode: Train env_steps 200 total rewards -1892.1898164749146 total energy tensor([[5.5631]])\n",
      "[-0.09633541]\n",
      "Mode: Train env_steps 200 total rewards -1706.1540986299515 total energy tensor([[18.4252]])\n",
      "[0.69708776]\n",
      "Mode: Train env_steps 200 total rewards -1807.896915435791 total energy tensor([[8.9913]])\n",
      "[-0.3820118]\n",
      "Mode: Train env_steps 200 total rewards -1830.5564813613892 total energy tensor([[7.6961]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30696237]\n",
      "Mode: Train env_steps 200 total rewards -1503.5457588669378 total energy tensor([[20.2857]])\n",
      "[-0.04197837]\n",
      "Mode: Train env_steps 200 total rewards -1573.6030056774616 total energy tensor([[20.4533]])\n",
      "[-0.9494325]\n",
      "Mode: Train env_steps 200 total rewards -1674.1722757816315 total energy tensor([[22.1644]])\n",
      "[0.17024739]\n",
      "Mode: Train env_steps 200 total rewards -1752.9832664728165 total energy tensor([[18.3063]])\n",
      "[-0.5231903]\n",
      "Mode: Train env_steps 200 total rewards -1846.5521125793457 total energy tensor([[6.8912]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33928522]\n",
      "Mode: Test env_steps 200 total rewards -1870.2622933387756 total energy tensor([[7.3857]])\n",
      "[0.21050774]\n",
      "Mode: Test env_steps 200 total rewards -1801.5695102214813 total energy tensor([[13.7577]])\n",
      "[0.38278642]\n",
      "Mode: Test env_steps 200 total rewards -1837.1514568328857 total energy tensor([[9.8529]])\n",
      "[0.00533392]\n",
      "Mode: Test env_steps 200 total rewards -1692.6143415868282 total energy tensor([[19.4232]])\n",
      "[-0.91372794]\n",
      "Mode: Test env_steps 200 total rewards -1750.1953234672546 total energy tensor([[19.1666]])\n",
      "[0.113194]\n",
      "Mode: Test env_steps 200 total rewards -1783.4181990623474 total energy tensor([[15.3524]])\n",
      "[0.666427]\n",
      "Mode: Test env_steps 200 total rewards -1787.567854642868 total energy tensor([[10.7574]])\n",
      "[-0.21509661]\n",
      "Mode: Test env_steps 200 total rewards -1890.2389907836914 total energy tensor([[5.8442]])\n",
      "[-0.66484404]\n",
      "Mode: Test env_steps 200 total rewards -1478.444243728707 total energy tensor([[20.2833]])\n",
      "[0.6756626]\n",
      "Mode: Test env_steps 200 total rewards -1888.0840549468994 total energy tensor([[5.9565]])\n",
      "90000 -1777.954626861174\n",
      "[-0.6329193]\n",
      "Mode: Train env_steps 200 total rewards -1891.7015771865845 total energy tensor([[5.4347]])\n",
      "[0.8602617]\n",
      "Mode: Train env_steps 200 total rewards -1891.8562774658203 total energy tensor([[5.3916]])\n",
      "[0.9939388]\n",
      "Mode: Train env_steps 200 total rewards -1851.7909746170044 total energy tensor([[6.9573]])\n",
      "[-0.930266]\n",
      "Mode: Train env_steps 200 total rewards -1799.6652946472168 total energy tensor([[13.0381]])\n",
      "[0.19114101]\n",
      "Mode: Train env_steps 200 total rewards -1631.2723717540503 total energy tensor([[20.3095]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:19<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20091054]\n",
      "Mode: Train env_steps 200 total rewards -1887.946605682373 total energy tensor([[6.3150]])\n",
      "[0.47487116]\n",
      "Mode: Train env_steps 200 total rewards -1844.9503064155579 total energy tensor([[7.3207]])\n",
      "[-0.03619559]\n",
      "Mode: Train env_steps 200 total rewards -1836.3171129226685 total energy tensor([[7.3694]])\n",
      "[0.90573007]\n",
      "Mode: Train env_steps 200 total rewards -1779.3033945560455 total energy tensor([[15.0833]])\n",
      "[0.64824486]\n",
      "Mode: Train env_steps 200 total rewards -1700.4361208379269 total energy tensor([[22.1868]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:31<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45997775]\n",
      "Mode: Train env_steps 200 total rewards -1841.0531311035156 total energy tensor([[9.3719]])\n",
      "[-0.01028112]\n",
      "Mode: Train env_steps 200 total rewards -1784.0215051174164 total energy tensor([[15.7810]])\n",
      "[-0.44098032]\n",
      "Mode: Train env_steps 200 total rewards -1814.889970779419 total energy tensor([[11.4618]])\n",
      "[0.21360171]\n",
      "Mode: Train env_steps 200 total rewards -1816.3333868980408 total energy tensor([[11.4164]])\n",
      "[-0.3464415]\n",
      "Mode: Train env_steps 200 total rewards -1869.5352816581726 total energy tensor([[5.6704]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:18<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.92728424]\n",
      "Mode: Train env_steps 200 total rewards -1866.0207705497742 total energy tensor([[7.5547]])\n",
      "[0.8080743]\n",
      "Mode: Train env_steps 200 total rewards -1830.0370125770569 total energy tensor([[7.6463]])\n",
      "[-0.50895125]\n",
      "Mode: Train env_steps 200 total rewards -1715.937338232994 total energy tensor([[18.6356]])\n",
      "[0.30430174]\n",
      "Mode: Train env_steps 200 total rewards -1731.3540697097778 total energy tensor([[20.4304]])\n",
      "[0.29450592]\n",
      "Mode: Train env_steps 200 total rewards -1817.0088455677032 total energy tensor([[12.2668]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:30<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3837602]\n",
      "Mode: Train env_steps 200 total rewards -1679.3960940390825 total energy tensor([[23.6428]])\n",
      "[-0.98358804]\n",
      "Mode: Train env_steps 200 total rewards -1745.8661725521088 total energy tensor([[19.4997]])\n",
      "[-0.2092879]\n",
      "Mode: Train env_steps 200 total rewards -1685.2538266181946 total energy tensor([[19.7271]])\n",
      "[-0.74188566]\n",
      "Mode: Train env_steps 200 total rewards -1867.89235496521 total energy tensor([[7.2867]])\n",
      "[-0.05028189]\n",
      "Mode: Train env_steps 200 total rewards -1821.2463307380676 total energy tensor([[8.1696]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.546985]\n",
      "Mode: Test env_steps 200 total rewards -1778.287343263626 total energy tensor([[15.3999]])\n",
      "[0.94923943]\n",
      "Mode: Test env_steps 200 total rewards -1831.101037979126 total energy tensor([[7.5752]])\n",
      "[0.19983184]\n",
      "Mode: Test env_steps 200 total rewards -1859.425280570984 total energy tensor([[7.3967]])\n",
      "[0.9870509]\n",
      "Mode: Test env_steps 200 total rewards -1819.9743347167969 total energy tensor([[12.0127]])\n",
      "[-0.65329236]\n",
      "Mode: Test env_steps 200 total rewards -1873.9424457550049 total energy tensor([[5.7577]])\n",
      "[0.876937]\n",
      "Mode: Test env_steps 200 total rewards -1776.951346874237 total energy tensor([[13.1152]])\n",
      "[-0.68259287]\n",
      "Mode: Test env_steps 200 total rewards -1638.4402808062732 total energy tensor([[23.4099]])\n",
      "[0.58076924]\n",
      "Mode: Test env_steps 200 total rewards -1837.8255515098572 total energy tensor([[7.4788]])\n",
      "[0.628061]\n",
      "Mode: Test env_steps 200 total rewards -1800.8119013309479 total energy tensor([[12.5725]])\n",
      "[-0.35442138]\n",
      "Mode: Test env_steps 200 total rewards -1874.2953691482544 total energy tensor([[5.5768]])\n",
      "95000 -1809.1054891955107\n",
      "[0.5275956]\n",
      "Mode: Train env_steps 200 total rewards -1765.5006767511368 total energy tensor([[18.5293]])\n",
      "[0.49203092]\n",
      "Mode: Train env_steps 200 total rewards -1796.9403836727142 total energy tensor([[14.2027]])\n",
      "[0.16072614]\n",
      "Mode: Train env_steps 200 total rewards -1823.480694770813 total energy tensor([[11.0940]])\n",
      "[0.16130769]\n",
      "Mode: Train env_steps 200 total rewards -1895.658133506775 total energy tensor([[4.9047]])\n",
      "[-0.7145829]\n",
      "Mode: Train env_steps 200 total rewards -1880.0184860229492 total energy tensor([[6.6508]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:29<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86512524]\n",
      "Mode: Train env_steps 200 total rewards -1789.3197169303894 total energy tensor([[13.8484]])\n",
      "[-0.63109]\n",
      "Mode: Train env_steps 200 total rewards -1519.6924812146463 total energy tensor([[20.9697]])\n",
      "[-0.33353096]\n",
      "Mode: Train env_steps 200 total rewards -1859.5119886398315 total energy tensor([[6.5010]])\n",
      "[-0.0985707]\n",
      "Mode: Train env_steps 200 total rewards -1731.6128414273262 total energy tensor([[19.7473]])\n",
      "[-0.675949]\n",
      "Mode: Train env_steps 200 total rewards -1737.2327086925507 total energy tensor([[17.7299]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9857002]\n",
      "Mode: Train env_steps 200 total rewards -1899.2441987991333 total energy tensor([[4.8139]])\n",
      "[-0.49577639]\n",
      "Mode: Train env_steps 200 total rewards -1725.244641304016 total energy tensor([[20.3798]])\n",
      "[0.399014]\n",
      "Mode: Train env_steps 200 total rewards -1734.7259806394577 total energy tensor([[20.5025]])\n",
      "[0.91409874]\n",
      "Mode: Train env_steps 200 total rewards -1734.2683662772179 total energy tensor([[19.2321]])\n",
      "[0.8428167]\n",
      "Mode: Train env_steps 200 total rewards -1889.470911026001 total energy tensor([[4.9363]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:22<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9689371]\n",
      "Mode: Train env_steps 200 total rewards -1815.825828075409 total energy tensor([[11.2904]])\n",
      "[0.11790281]\n",
      "Mode: Train env_steps 200 total rewards -1804.0283613204956 total energy tensor([[9.8162]])\n",
      "[0.516075]\n",
      "Mode: Train env_steps 200 total rewards -1870.0786681175232 total energy tensor([[5.8449]])\n",
      "[0.7338468]\n",
      "Mode: Train env_steps 200 total rewards -1577.181947948411 total energy tensor([[22.8520]])\n",
      "[-0.23374926]\n",
      "Mode: Train env_steps 200 total rewards -1899.1533155441284 total energy tensor([[4.7694]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:20<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.88392764]\n",
      "Mode: Train env_steps 200 total rewards -1705.887121528387 total energy tensor([[21.8451]])\n",
      "[0.84711874]\n",
      "Mode: Train env_steps 200 total rewards -1812.7859234809875 total energy tensor([[8.6394]])\n",
      "[-0.3245]\n",
      "Mode: Train env_steps 200 total rewards -1886.309509754181 total energy tensor([[5.6477]])\n",
      "[0.23821297]\n",
      "Mode: Train env_steps 200 total rewards -1722.3111329078674 total energy tensor([[21.0893]])\n",
      "[-0.03957774]\n",
      "Mode: Train env_steps 200 total rewards -1718.103648751974 total energy tensor([[21.3607]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:34<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44239423]\n",
      "Mode: Test env_steps 200 total rewards -1903.939073562622 total energy tensor([[4.0020]])\n",
      "[0.06805965]\n",
      "Mode: Test env_steps 200 total rewards -1387.8616145834676 total energy tensor([[20.4449]])\n",
      "[0.29755518]\n",
      "Mode: Test env_steps 200 total rewards -1891.3293857574463 total energy tensor([[5.6607]])\n",
      "[-0.08687387]\n",
      "Mode: Test env_steps 200 total rewards -1808.00324344635 total energy tensor([[9.4601]])\n",
      "[0.7796385]\n",
      "Mode: Test env_steps 200 total rewards -1737.6727023124695 total energy tensor([[19.4286]])\n",
      "[0.2572593]\n",
      "Mode: Test env_steps 200 total rewards -1736.162268280983 total energy tensor([[20.4354]])\n",
      "[-0.6891838]\n",
      "Mode: Test env_steps 200 total rewards -1640.7262412682176 total energy tensor([[22.9375]])\n",
      "[0.94457126]\n",
      "Mode: Test env_steps 200 total rewards -1700.144299775362 total energy tensor([[21.2927]])\n",
      "[0.55290264]\n",
      "Mode: Test env_steps 200 total rewards -1858.2984099388123 total energy tensor([[5.8093]])\n",
      "[0.69903004]\n",
      "Mode: Test env_steps 200 total rewards -1819.502124786377 total energy tensor([[11.5544]])\n",
      "100000 -1748.3639363712107\n",
      "[-0.68567836]\n",
      "Mode: Train env_steps 200 total rewards -1864.524863243103 total energy tensor([[5.3810]])\n",
      "[-0.34937754]\n",
      "Mode: Train env_steps 200 total rewards -1740.3697412014008 total energy tensor([[20.5773]])\n",
      "[-0.7408097]\n",
      "Mode: Train env_steps 200 total rewards -1756.3044846057892 total energy tensor([[17.5283]])\n",
      "[-0.13947774]\n",
      "Mode: Train env_steps 200 total rewards -1776.5995037555695 total energy tensor([[16.9081]])\n",
      "[0.41207534]\n",
      "Mode: Train env_steps 200 total rewards -1734.5718858242035 total energy tensor([[16.3958]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:29<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6783426]\n",
      "Mode: Train env_steps 200 total rewards -1782.638923048973 total energy tensor([[16.2449]])\n",
      "[-0.13358653]\n",
      "Mode: Train env_steps 200 total rewards -1798.5296258926392 total energy tensor([[11.0895]])\n",
      "[0.92208856]\n",
      "Mode: Train env_steps 200 total rewards -1917.99906539917 total energy tensor([[2.6951]])\n",
      "[0.79094696]\n",
      "Mode: Train env_steps 200 total rewards -1751.598008275032 total energy tensor([[17.7042]])\n",
      "[-0.65157354]\n",
      "Mode: Train env_steps 200 total rewards -1689.3832034021616 total energy tensor([[21.0657]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [00:56<00:43,  3.92s/it]"
     ]
    }
   ],
   "source": [
    "policy_storage = SeqReplayBuffer(\n",
    "    max_replay_buffer_size=buffer_size,\n",
    "    observation_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    sampled_seq_len=sampled_seq_len,\n",
    "    sample_weight_baseline=0.0,\n",
    ")\n",
    "\n",
    "env_steps = collect_rollouts(\n",
    "    num_rollouts=num_init_rollouts_pool, random_actions=False, train_mode=True\n",
    ")\n",
    "_n_env_steps_total += env_steps\n",
    "\n",
    "# evaluation parameters\n",
    "last_eval_num_iters = 10\n",
    "log_interval = 5\n",
    "eval_num_rollouts = 10\n",
    "learning_curve = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"z\": [],\n",
    "}\n",
    "epoch=0\n",
    "lambda_pat = 0.65\n",
    "\n",
    "while _n_env_steps_total < n_env_steps_total:\n",
    "\n",
    "    env_steps = collect_rollouts(num_rollouts=num_rollouts_per_iter, train_mode=True)\n",
    "    _n_env_steps_total += env_steps\n",
    "\n",
    "    #train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "    factor= lambda_pat **(epoch )\n",
    "    #train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "    train_stats = update(25, lr)\n",
    "    \n",
    "    epoch += 1\n",
    "    current_num_iters = _n_env_steps_total // (\n",
    "        num_rollouts_per_iter * max_trajectory_len\n",
    "    )\n",
    "    if (\n",
    "        current_num_iters != last_eval_num_iters\n",
    "        and current_num_iters % log_interval == 0\n",
    "    ):\n",
    "        last_eval_num_iters = current_num_iters\n",
    "        average_returns, std_returns = collect_rollouts(\n",
    "            num_rollouts=eval_num_rollouts,\n",
    "            train_mode=False,\n",
    "            random_actions=False,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        learning_curve[\"x\"].append(_n_env_steps_total)\n",
    "        learning_curve[\"y\"].append(average_returns)\n",
    "        learning_curve[\"z\"].append(std_returns)\n",
    "        print(_n_env_steps_total, average_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(learning_curve)\n",
    "plt.plot(learning_curve[\"x\"], learning_curve[\"y\"])\n",
    "plt.fill_between(np.array(learning_curve[\"x\"]), np.array(learning_curve[\"y\"])-np.array(learning_curve[\"z\"]), np.array(learning_curve[\"y\"])+np.array(learning_curve[\"z\"]))\n",
    "plt.xlabel(\"env steps\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaning_curve_ncde_64_rk4 = learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timess=torch.linspace(0, 65-1, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('config.txt', 'w')\n",
    "file1.write(str(conf))\n",
    "\n",
    "file1.close()\n",
    "file2 = open('results.txt', 'w')\n",
    "file2.write(str(learning_curve))\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5f49fb63f78fde0f27b95a7c8e14eeaa9af6d816174ff450f7bbbcd21c7c97c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
